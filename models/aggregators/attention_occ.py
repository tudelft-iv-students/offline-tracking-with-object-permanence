import torch
import torch.nn as nn
from models.aggregators.aggregator import PredictionAggregator
from typing import Dict, Tuple
from models.library.blocks import *


class Attention_occ(PredictionAggregator):
    """
    Aggregate context encoding using scaled dot product attention. Query obtained using target agent encoding,
    Keys and values obtained using map and surrounding agent encodings.
    """

    def __init__(self, args: Dict):

        """
        args to include

        enc_size: int Dimension of encodings generated by encoder
        emb_size: int Size of embeddings used for queries, keys and values
        num_heads: int Number of attention heads

        """
        super().__init__()
        self.tq_embder=nn.Sequential(nn.Linear(2,args['time_emb_size']),nn.LeakyReLU())
        self.method=args['method']
        self.concat_latest = args['concat_latest']
        if self.concat_latest:
            if self.method == 'attention':
                self.tq_attn = nn.MultiheadAttention(args['attn_size'], 1)
                self.query_emb = nn.Sequential(nn.Linear(2, args['attn_size']),nn.LeakyReLU())
                self.key_emb = nn.Sequential(nn.Linear(2, args['attn_size']),nn.LeakyReLU())
                self.val_emb = nn.Sequential(nn.Linear(args['target_emb_size']+3, args['attn_size']),nn.LeakyReLU())
                self.mlp_agg = leaky_MLP(args['query_enc_size']+args['attn_size'],args['query_enc_size'])
            
            self.compressor = leaky_MLP((args['time_emb_size']+2*(args['target_emb_size']+3)),args['query_enc_size'])
        else:
            if self.method == 'attention':
                self.tq_attn = nn.MultiheadAttention(args['attn_size'], 1)
                self.query_emb = nn.Sequential(nn.Linear(2, args['attn_size']),nn.LeakyReLU())
                self.key_emb = nn.Sequential(nn.Linear(2, args['attn_size']),nn.LeakyReLU())
                self.val_emb = nn.Sequential(nn.Linear(args['target_emb_size'], args['attn_size']),nn.LeakyReLU())
                self.mlp_agg = leaky_MLP(args['query_enc_size']+args['attn_size'],args['query_enc_size'])
            
            self.compressor = leaky_MLP((args['time_emb_size']+2*args['target_emb_size']),args['query_enc_size'])

    def forward(self, encodings: Dict) -> torch.Tensor:
        """
        Forward pass for attention aggregator
        """
        target_representation=encodings['target_agent_encoding']
        target_hist=target_representation['hist']
        target_future=target_representation['future']
        time_query=target_representation['time_query']['query']
        mask=target_representation['time_query']['mask']
        query_emb=self.tq_embder(time_query)
        concat_tgt_enc=torch.cat((target_hist,target_future),dim=-1).repeat(1,query_emb.shape[1],1)
        concat_query=self.compressor(torch.cat((query_emb,concat_tgt_enc),dim=-1))
        if 'endpoints' in target_representation['time_query']:
            endpt_query=target_representation['time_query']['endpoints']
            endpt_query_emb=self.tq_embder(endpt_query)
            concat_tgt_enc_endpt=torch.cat((target_hist,target_future),dim=-1).repeat(1,endpt_query_emb.shape[1],1)
            concat_endpt_query=self.compressor(torch.cat((endpt_query_emb,concat_tgt_enc_endpt),dim=-1))
        if self.method == 'attention':
            # q_t_attn_masks = mask.bool()
            feature=torch.cat((target_hist,target_future),dim=1)
            attn_query=self.query_emb(time_query).permute(1,0,2)
            key=self.key_emb(endpt_query).permute(1,0,2)
            val=self.val_emb(feature).permute(1,0,2)
            t_q_att_op, _ = self.tq_attn(attn_query, key, val)
            t_q_att_op=t_q_att_op.transpose(0,1)
            # t_q_att_op = torch.cat((t_q_att_op, time_query), dim=-1)
            agg_feature=self.mlp_agg(torch.cat((t_q_att_op,concat_query),dim=-1))
            time_query_enc={'query':agg_feature,'mask':mask}
            if 'endpoints' in target_representation['time_query']:
                ep_query_emb=self.query_emb(endpt_query).permute(1,0,2)
                ep_map_key=self.key_emb(endpt_query).permute(1,0,2)
                ep_map_val=self.val_emb(feature).permute(1,0,2)
                ep_att_op, _ = self.tq_attn(ep_query_emb, ep_map_key, ep_map_val)
                ep_att_op=ep_att_op.transpose(0,1)
                # t_q_att_op = torch.cat((t_q_att_op, time_query), dim=-1)
                ep_agg_feature=self.mlp_agg(torch.cat((ep_att_op,concat_endpt_query),dim=-1))
                time_query_enc['endpoints']=ep_agg_feature
        else:
            time_query_enc={'query':concat_query,'mask':mask}
            if 'endpoints' in target_representation['time_query']:
                time_query_enc['endpoints']=concat_endpt_query
        if 'lane_info' in encodings['context_encoding']:
            time_query_enc['lane_info']=encodings['context_encoding']['lane_info']
        if 'lane_ctrs' in encodings['context_encoding']:
            time_query_enc['lane_ctrs']=encodings['context_encoding']['lane_ctrs']
        if 'nbr_info' in encodings['context_encoding']:
            time_query_enc['nbr_info']=encodings['context_encoding']['nbr_info']
        if 'refine_input' in encodings:
            time_query_enc['refine_input'] = encodings['refine_input']
        return time_query_enc

    @staticmethod
    def get_combined_encodings(context_enc: Dict) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Creates a combined set of map and surrounding agent encodings to be aggregated using attention.
        """
        encodings = []
        masks = []
        if 'map' in context_enc:
            encodings.append(context_enc['map'])
            masks.append(context_enc['map_masks'])
        if 'vehicles' in context_enc:
            encodings.append(context_enc['vehicles'])
            masks.append(context_enc['vehicle_masks'])
        if 'pedestrians' in context_enc:
            encodings.append(context_enc['pedestrians'])
            masks.append(context_enc['pedestrian_masks'])
        combined_enc = torch.cat(encodings, dim=1)
        combined_masks = torch.cat(masks, dim=1).bool()
        return combined_enc, combined_masks
