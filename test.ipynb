{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "# from train_eval.trainer import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/occlusion_train_v2_vis.yml\", 'r') as yaml_file:\n",
    "    cfg = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_eval.evaluator import Evaluator\n",
    "trainer = Evaluator(cfg, \"/home/stanliu/data/mnt/nuScenes/nuscenes\", \"vis_data\",\"track_completion_model/track_completion_att2.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "from typing import Dict, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets.nuScenes.prediction import PredictHelper_occ\n",
    "from nuscenes.prediction.input_representation.static_layers import *\n",
    "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.eval.common.utils import quaternion_yaw\n",
    "import logging\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "from PIL import Image\n",
    "import copy\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from executables.track_completion import *\n",
    "def get_cam(sample_record,nusc,anntoken):\n",
    "    cams = [key for key in sample_record['data'].keys() if 'CAM' in key]\n",
    "    for cam in cams:\n",
    "        _, boxes, _ = nusc.get_sample_data(sample_record['data'][cam], box_vis_level=BoxVisibility.ANY,\n",
    "                                                selected_anntokens=[anntoken])\n",
    "        if len(boxes) > 0:\n",
    "            break  # We found an image that matches. Let's abort.\n",
    "    assert len(boxes) > 0, 'Error: Could not find image where annotation is visible. ' \\\n",
    "                            'Try using e.g. BoxVisibility.ANY.'\n",
    "    assert len(boxes) < 2, 'Error: Found multiple annotations. Something is wrong!'\n",
    "\n",
    "    cam = sample_record['data'][cam]\n",
    "    return cam\n",
    "def render_occ_anns(frame,nusc: NuScenes,global_coord,global_rotation,save_dir,key,frame_id,save):\n",
    "    anntoken=frame['ann_token']\n",
    "    ann_record = nusc.get('sample_annotation', anntoken)\n",
    "    sample_record = nusc.get('sample', ann_record['sample_token'])\n",
    "    cam = get_cam(sample_record,nusc,anntoken)\n",
    "    z=nusc.get('sample_annotation', anntoken)['translation'][-1]\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    sd_record = nusc.get('sample_data', cam)\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    # sensor_record = nusc.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "\n",
    "    im = Image.open(data_path)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(9, 9))\n",
    "    axes.imshow(im)\n",
    "    axes.set_title(nusc.get('sample_data', cam)['channel'])\n",
    "    axes.axis('off')\n",
    "    axes.set_aspect('equal')\n",
    "    def get_color(name):\n",
    "        return nusc.colormap[name]\n",
    "    assert len(boxes)==1\n",
    "    pred_box=copy.deepcopy(boxes[0])\n",
    "    pred_box.name=\"human.pedestrian.adult\"\n",
    "    pred_box.center=global_coord+[z]\n",
    "    pred_box.orientation=Quaternion(global_rotation)\n",
    "\n",
    "    pred_box.translate(-np.array(pose_record['translation']))\n",
    "    pred_box.rotate(Quaternion(pose_record['rotation']).inverse)\n",
    "\n",
    "    #  Move box to sensor coord system.\n",
    "    pred_box.translate(-np.array(cs_record['translation']))\n",
    "    pred_box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "    boxes.append(pred_box)\n",
    "    # boxes.append()\n",
    "    for box in boxes:\n",
    "        c = np.array(get_color(box.name)) / 255.0\n",
    "        box.render(axes, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "    if save:\n",
    "        out_dir=os.path.join(save_dir,key)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path=os.path.join(out_dir,str(frame_id))\n",
    "        plt.savefig(out_path)\n",
    "        plt.close(fig)\n",
    "    return\n",
    "\n",
    "@torch.no_grad()\n",
    "def local_pose_to_image(local_poses,pose_mask,resolution,img_size,arror_length=None):\n",
    "    '''local_poses: [T,4] \n",
    "    mask: [T]\n",
    "    '''\n",
    "    if arror_length is None:\n",
    "        arror_length=8\n",
    "    y_m=np.asarray(local_poses[:,1][pose_mask].cpu())\n",
    "    x_m=np.asarray(local_poses[:,0][pose_mask].cpu())\n",
    "    img_origin=np.round(np.asarray(img_size)/2).astype(np.int)\n",
    "    x=img_origin[1]+x_m*resolution\n",
    "    y=img_origin[0]-y_m*resolution\n",
    "    yaw=np.asarray((local_poses[:,2][pose_mask]).cpu())\n",
    "    dy=-np.sin(yaw+np.pi/2)*arror_length\n",
    "    dx=np.cos(yaw+np.pi/2)*arror_length\n",
    "    return x,y,dx,dy\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(inputs: Dict,ground_truth: Dict,predictions: Dict,helper: PredictHelper_occ, selector,token_dicts,save_folder='./tmp', mode='refine',save=False):\n",
    "\n",
    "    upper_limit=100\n",
    "    batch_size=len(predictions['traj'])\n",
    "    layer_names = ['drivable_area', 'ped_crossing']\n",
    "    maps= load_all_maps(helper)\n",
    "    colors = [(255, 255, 255), (119, 136, 153)]\n",
    "    for sample_id in range(batch_size):\n",
    "        if sample_id>upper_limit:\n",
    "            return\n",
    "        # try:\n",
    "        if not selector[sample_id]:\n",
    "            continue\n",
    "        instance_token=inputs['instance_token'][sample_id]\n",
    "        sample_token=inputs['sample_token'][sample_id]\n",
    "        future=inputs['target_agent_representation']['future']['traj'][sample_id]\n",
    "        mask_fut=inputs['target_agent_representation']['future']['mask'][sample_id]\n",
    "        hist=inputs['target_agent_representation']['history']['traj'][sample_id]\n",
    "        nearest_idx=np.where(mask_fut[:, 0].cpu() == 0)[0][-1]\n",
    "        prediction_horizon=future[nearest_idx,-1]\n",
    "        sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "            \n",
    "        map_name = helper.get_map_name_from_sample_token(sample_token)\n",
    "        x, y = sample_annotation['translation'][:2]\n",
    "        yaw = quaternion_yaw(Quaternion(sample_annotation['rotation']))\n",
    "        yaw_corrected = correct_yaw(yaw)\n",
    "        global_pose=(x,y,yaw_corrected)\n",
    "        if 'origin' in inputs:\n",
    "            origin=tuple([inputs['origin'][sample_id,0].item(),inputs['origin'][sample_id,1].item(),inputs['origin'][sample_id,2].item()])\n",
    "        else:\n",
    "            coords_fut,global_yaw_fut,time_fut = helper.get_future_for_agent(instance_token, sample_token, seconds=2+prediction_horizon, in_agent_frame=False,add_yaw_and_time=True)\n",
    "\n",
    "            sep_idx= np.searchsorted(time_fut, (prediction_horizon-0.001).item())\n",
    "            origin_fut=coords_fut[sep_idx][0],coords_fut[sep_idx][1],correct_yaw(quaternion_yaw(Quaternion(global_yaw_fut[sep_idx])))\n",
    "            origin=tuple((np.asarray(global_pose)+np.asarray(origin_fut))/2)\n",
    "        dist=LA.norm(future[0,:2].cpu(),ord=2)\n",
    "        image_side_length = 2 * max(25,dist+10)\n",
    "        image_side_length_pixels = 400\n",
    "        resolution=image_side_length_pixels/image_side_length\n",
    "        patchbox = get_patchbox(origin[0], origin[1], image_side_length)\n",
    "\n",
    "        angle_in_degrees = angle_of_rotation(origin[2]) * 180 / np.pi\n",
    "\n",
    "        canvas_size = (image_side_length_pixels, image_side_length_pixels)\n",
    "        masks = maps[map_name].get_map_mask(patchbox, angle_in_degrees, layer_names, canvas_size=canvas_size)\n",
    "        \n",
    "        images = []\n",
    "        for mask, color in zip(masks, colors):\n",
    "            images.append(change_color_of_binary_mask(np.repeat(mask[::-1, :, np.newaxis], 3, 2), color))\n",
    "        if mode=='refine':\n",
    "            traj = predictions['refined_traj'][sample_id].squeeze(0)\n",
    "            yaw = predictions['refined_yaw'][sample_id]\n",
    "        elif mode=='raw':\n",
    "            traj = predictions['traj'][sample_id].squeeze(0)\n",
    "            yaw = predictions['yaw'][sample_id]\n",
    "        lanes=inputs['map_representation']['lane_node_feats'][sample_id].flatten(0,1).clone()\n",
    "        lanes_mask=inputs['map_representation']['lane_node_feats'][sample_id].flatten(0,1)[:,0].bool()\n",
    "        pred = torch.cat((traj,yaw),-1)\n",
    "        pose_pred_mask=~(predictions['mask'][sample_id]).bool()\n",
    "        gt = ground_truth['traj'][sample_id]\n",
    "        image = Rasterizer().combine(images)\n",
    "        pose_future_mask=~inputs['target_agent_representation']['future']['mask'][sample_id][:,0].bool()\n",
    "        pose_hist_mask=~inputs['target_agent_representation']['history']['mask'][sample_id][:,0].bool()\n",
    "        xs, ys, dxs, dys=local_pose_to_image(future,pose_future_mask,resolution,canvas_size)\n",
    "        xsh, ysh, dxsh, dysh=local_pose_to_image(hist,pose_hist_mask,resolution,canvas_size)\n",
    "        xsp, ysp, dxsp, dysp=local_pose_to_image(pred,pose_pred_mask,resolution,canvas_size,5)\n",
    "        xsg, ysg, dxsg, dysg=local_pose_to_image(gt,pose_pred_mask,resolution,canvas_size,5)\n",
    "        xsl, ysl, dxsl, dysl=local_pose_to_image(lanes,lanes_mask,resolution,canvas_size,2.5)\n",
    "        # plt.imshow(image)\n",
    "        fig = plt.figure(figsize = (8,8))\n",
    "        ax = fig.add_subplot(1,1,1) \n",
    "        \n",
    "        for x, y, dx, dy in zip(xs, ys, dxs, dys):\n",
    "            ax.arrow(x, y, dx, dy, width=0.8, color=(1,0,0,1))\n",
    "        for x, y, dx, dy in zip(xsh, ysh, dxsh, dysh):\n",
    "            ax.arrow(x, y, dx, dy, width=0.8, color=(0,1,0,1))\n",
    "        for x, y, dx, dy in zip(xsp, ysp, dxsp, dysp):\n",
    "            ax.arrow(x, y, dx, dy, width=1.0, color=(0.0,0,1,1))\n",
    "        for x, y, dx, dy in zip(xsg, ysg, dxsg, dysg):\n",
    "            ax.arrow(x, y, dx, dy, width=1.0, color=(1,0,1,0.3))\n",
    "        for x, y, dx, dy in zip(xsl, ysl, dxsl, dysl):\n",
    "            ax.arrow(x, y, dx, dy, width=0.5, color=(1,0.5,0,0.3))\n",
    "        ax.imshow(image)\n",
    "        ax.grid(False)\n",
    "        fig.canvas.draw()\n",
    "        image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        # plt.close(fig)\n",
    "        if save:\n",
    "            plt.close(fig)\n",
    "            out_dir=os.path.join(save_folder,instance_token+'_'+sample_token)\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            im.imsave(os.path.join(out_dir,'bev'), image_from_plot)\n",
    "            # except:\n",
    "            #     continue\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "        key=instance_token+\"_\"+sample_token\n",
    "\n",
    "        missing_frames=token_dicts[key]\n",
    "        for idx,frame in enumerate(missing_frames):\n",
    "            yaw=pred[idx,-1].item()\n",
    "\n",
    "            global_coord=local_to_global(origin, tuple(pred[idx,:2].cpu().numpy()))\n",
    "            global_rotation=get_global_rotation(origin,yaw)\n",
    "\n",
    "            render_occ_anns(frame,helper.data,list(global_coord),global_rotation,save_folder,key,idx,save)\n",
    "        # break\n",
    "    return\n",
    "@torch.no_grad()\n",
    "def visualize_preparations(vis_dict, inputs: Dict,ground_truth: Dict,predictions: Dict,helper: PredictHelper_occ, selector,token_dicts,save_folder='./tmp', mode='refine',save=False):\n",
    "\n",
    "    batch_size=len(predictions['traj'])\n",
    "    for sample_id in range(batch_size):\n",
    "        if not selector[sample_id]:\n",
    "            continue\n",
    "        instance_token=inputs['instance_token'][sample_id]\n",
    "        sample_token=inputs['sample_token'][sample_id]\n",
    "        key=instance_token+\"_\"+sample_token\n",
    "        \n",
    "        future=inputs['target_agent_representation']['future']['traj'][sample_id]\n",
    "        mask_fut=inputs['target_agent_representation']['future']['mask'][sample_id]\n",
    "        hist=inputs['target_agent_representation']['history']['traj'][sample_id]\n",
    "        vis_dict[key]={\n",
    "            \"future\":future,\n",
    "            \"mask_fut\":mask_fut,\n",
    "            \"history\":hist,\n",
    "            \"origin\":inputs['origin'][sample_id],\n",
    "            \"traj\" :predictions['refined_traj'][sample_id],\n",
    "            \"yaw\" :predictions['refined_yaw'][sample_id],\n",
    "            \"pose_pred_mask\":predictions['mask'][sample_id],\n",
    "            \"gt\" : ground_truth['traj'][sample_id],\n",
    "            \"lanes\":inputs['map_representation']['lane_node_feats'][sample_id],\n",
    "            \"lanes_mask\":inputs['map_representation']['lane_node_masks'][sample_id],\n",
    "            \"pose_future_mask\":inputs['target_agent_representation']['future']['mask'][sample_id],\n",
    "            \"pose_hist_mask\":inputs['target_agent_representation']['history']['mask'][sample_id]\n",
    "        }\n",
    "\n",
    "    return vis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list=trainer.dl.dataset.data_list\n",
    "token_dicts={}\n",
    "full_token_dicts={}\n",
    "for token_dict in tokens_list:\n",
    "    key=token_dict['start']['ins_token']+\"_\"+token_dict['start']['sample_token']\n",
    "    token_dicts[key]=token_dict['missing_frames']\n",
    "    full_token_dicts[key]=token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_eval.utils as u\n",
    "helper= PredictHelper_occ(trainer.dl.dataset.helper.data)\n",
    "vis_dicts={}\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(trainer.dl):\n",
    "        print(i,len(trainer.dl))\n",
    "        # torch.cuda.empty_cache()\n",
    "        # Load data\n",
    "        data = u.send_to_device(u.convert_double_to_float(data))\n",
    "        data_test=data['inputs']\n",
    "        gt_test=data['ground_truth']\n",
    "        \n",
    "        selectors=(torch.norm(gt_test['traj'][:,:,-1],1,dim=1).cpu()>1.5)*(torch.sum(torch.norm(gt_test['traj'][:,:,:2],2,dim=-1),dim=-1).cpu()>10.0)\n",
    "        predcitions=trainer.model(data_test)\n",
    "        # visualize(data_test,gt_test,predcitions,helper, selectors, token_dicts,save=True)\n",
    "        vis_dicts=visualize_preparations(vis_dicts, data_test, gt_test, predcitions, helper, selectors, token_dicts)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_sample_token=\"d45c8e8a7e9e45458003153e96fa3259_f9475d1f1f96401c81ce511a93578a96\"\n",
    "vis_dict=vis_dicts[vis_sample_token]\n",
    "missing_frames=token_dicts[vis_sample_token]\n",
    "token_dict=full_token_dicts[vis_sample_token]\n",
    "hist_frames=3\n",
    "fut_frames=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_names = ['drivable_area', 'ped_crossing']\n",
    "maps= load_all_maps(helper)\n",
    "colors = [(255, 255, 255), (119, 136, 153)]\n",
    "instance_token,sample_token=vis_sample_token.split(\"_\")\n",
    "start_ann_token=token_dict['start']['ann_token']\n",
    "hist_tokens=[start_ann_token]\n",
    "end_ann_token=token_dict['end']['ann_token']\n",
    "fut_tokens=[end_ann_token]\n",
    "metadata=helper.data.get('sample_annotation',start_ann_token)\n",
    "for idx in range(hist_frames-1):\n",
    "    if metadata['prev'] == '':\n",
    "        break\n",
    "    hist_token=metadata['prev']\n",
    "    hist_tokens.append(hist_token)\n",
    "    metadata=helper.data.get('sample_annotation',hist_token)\n",
    "# hist_tokens=hist_tokens\n",
    "metadata=helper.data.get('sample_annotation',end_ann_token)\n",
    "for idx in range(fut_frames-1):\n",
    "    if metadata['next'] == '':\n",
    "        break\n",
    "    fut_token=metadata['next']\n",
    "    fut_tokens.append(fut_token)\n",
    "    metadata=helper.data.get('sample_annotation',fut_token)\n",
    "# fut_tokens=fut_tokens[::-1]\n",
    "future=vis_dict['future']\n",
    "mask_fut=vis_dict['mask_fut']\n",
    "hist=vis_dict['history']\n",
    "nearest_idx=np.where(mask_fut[:, 0].cpu() == 0)[0][-1]\n",
    "prediction_horizon=future[nearest_idx,-1]\n",
    "sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "    \n",
    "map_name = helper.get_map_name_from_sample_token(sample_token)\n",
    "x, y = sample_annotation['translation'][:2]\n",
    "yaw = quaternion_yaw(Quaternion(sample_annotation['rotation']))\n",
    "yaw_corrected = correct_yaw(yaw)\n",
    "global_pose=(x,y,yaw_corrected)\n",
    "origin=tuple([vis_dict['origin'][0].item(),vis_dict['origin'][1].item(),vis_dict['origin'][2].item()])\n",
    "dist=LA.norm(future[0,:2].cpu(),ord=2)\n",
    "image_side_length = 2 * max(25,dist+10)\n",
    "image_side_length_pixels = 1200\n",
    "resolution=image_side_length_pixels/image_side_length\n",
    "patchbox = get_patchbox(origin[0], origin[1], image_side_length)\n",
    "\n",
    "angle_in_degrees = angle_of_rotation(origin[2]) * 180 / np.pi\n",
    "\n",
    "canvas_size = (image_side_length_pixels, image_side_length_pixels)\n",
    "masks = maps[map_name].get_map_mask(patchbox, angle_in_degrees, layer_names, canvas_size=canvas_size)\n",
    "\n",
    "images = []\n",
    "for mask, color in zip(masks, colors):\n",
    "    images.append(change_color_of_binary_mask(np.repeat(mask[::-1, :, np.newaxis], 3, 2), color))\n",
    "\n",
    "traj = vis_dict['traj'].squeeze(0)\n",
    "yaw = vis_dict['yaw']\n",
    "lanes=vis_dict['lanes'].flatten(0,1).clone()\n",
    "lanes_mask=~vis_dict['lanes_mask'].flatten(0,1)[:,0].bool()\n",
    "pred = torch.cat((traj,yaw),-1)\n",
    "pose_pred_mask=~(vis_dict[\"pose_pred_mask\"]).bool()\n",
    "gt = vis_dict[\"gt\"]\n",
    "image = Rasterizer().combine(images)\n",
    "pose_future_mask=~vis_dict[\"pose_future_mask\"][:,0].bool()\n",
    "pose_hist_mask=~vis_dict[\"pose_hist_mask\"][:,0].bool()\n",
    "xs, ys, dxs, dys=local_pose_to_image(future,pose_future_mask,resolution,canvas_size,30)\n",
    "xsh, ysh, dxsh, dysh=local_pose_to_image(hist,pose_hist_mask,resolution,canvas_size,30)\n",
    "xsp, ysp, dxsp, dysp=local_pose_to_image(pred,pose_pred_mask,resolution,canvas_size,30)\n",
    "xsg, ysg, dxsg, dysg=local_pose_to_image(gt,pose_pred_mask,resolution,canvas_size,30)\n",
    "xsl, ysl, dxsl, dysl=local_pose_to_image(lanes,lanes_mask,resolution,canvas_size,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xshr=xsh[::-1]\n",
    "yshr=ysh[::-1]\n",
    "dxshr=dxsh[::-1]\n",
    "dyshr=dysh[::-1]\n",
    "# xshr=xsh\n",
    "# yshr=ysh\n",
    "# dxshr=dxsh\n",
    "# dyshr=dysh\n",
    "hist_imgs=[]\n",
    "nusc=trainer.dl.dataset.helper.data\n",
    "for frame_id in range(len(xshr)):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(40, 20))\n",
    "    anntoken=hist_tokens[frame_id]\n",
    "    ann_record = nusc.get('sample_annotation', anntoken)\n",
    "    sample_record = nusc.get('sample', ann_record['sample_token'])\n",
    "    cam = get_cam(sample_record,nusc,anntoken)\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    sd_record = nusc.get('sample_data', cam)\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    # sensor_record = nusc.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "    im = Image.open(data_path)\n",
    "\n",
    "    axes[1].imshow(im)\n",
    "    axes[1].set_title('Frame '+ str(hist_frames-frame_id) + \"    Status: Visible\", fontsize=35)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_aspect('equal')\n",
    "    def get_color(name):\n",
    "        return nusc.colormap[name]\n",
    "    assert len(boxes)==1\n",
    "    \n",
    "    for box in boxes:\n",
    "        c = np.array(get_color(box.name)) / 255.0\n",
    "        box.render(axes[1], view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "    \n",
    "    x=xshr[frame_id]\n",
    "    y=yshr[frame_id]\n",
    "    dx=dxshr[frame_id]\n",
    "    dy=dyshr[frame_id]\n",
    "    axes[0].arrow(x, y, dx, dy, width=6.0, color=(1.        , 0.38823529, 0.27843137,1))\n",
    "\n",
    "    for x, y, dx, dy in zip(xsl, ysl, dxsl, dysl):\n",
    "        axes[0].arrow(x, y, dx, dy, width=1.0, color=(1,0.5,0,0.3))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].grid(False)\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    hist_imgs.append(image_from_plot)\n",
    "hist_imgs=hist_imgs[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xspr=xsp[::-1]\n",
    "# yspr=ysp[::-1]\n",
    "# dxspr=dxsp[::-1]\n",
    "# dyspr=dysp[::-1]\n",
    "xspr=xsp\n",
    "yspr=ysp\n",
    "dxspr=dxsp\n",
    "dyspr=dysp\n",
    "pred_imgs=[]\n",
    "nusc=trainer.dl.dataset.helper.data\n",
    "for frame_id in range(len(xspr)):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(40, 20))\n",
    "    anntoken=missing_frames[frame_id]['ann_token']\n",
    "    ann_record = nusc.get('sample_annotation', anntoken)\n",
    "    sample_record = nusc.get('sample', ann_record['sample_token'])\n",
    "    cam = get_cam(sample_record,nusc,anntoken)\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    sd_record = nusc.get('sample_data', cam)\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    # sensor_record = nusc.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "    im = Image.open(data_path)\n",
    "\n",
    "    axes[1].imshow(im)\n",
    "    axes[1].set_title('Frame '+ str(frame_id+hist_frames+1) + \"    Status: Occluded\", fontsize=35)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_aspect('equal')\n",
    "    def get_color(name):\n",
    "        return nusc.colormap[name]\n",
    "    assert len(boxes)==1\n",
    "    \n",
    "    for box in boxes:\n",
    "        box.name=\"human.pedestrian.adult\"\n",
    "        c = np.array(get_color(box.name)) / 255.0\n",
    "        box.render(axes[1], view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "    \n",
    "    x=xspr[frame_id]\n",
    "    y=yspr[frame_id]\n",
    "    dx=dxspr[frame_id]\n",
    "    dy=dyspr[frame_id]\n",
    "    axes[0].arrow(x, y, dx, dy, width=6.0, color=(0.        , 0.        , 0.90196078,1))\n",
    "\n",
    "    for x, y, dx, dy in zip(xsl, ysl, dxsl, dysl):\n",
    "        axes[0].arrow(x, y, dx, dy, width=1.0, color=(1,0.5,0,0.3))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].grid(False)\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    pred_imgs.append(image_from_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsfr=xs[::-1]\n",
    "ysfr=ys[::-1]\n",
    "dxsfr=dxs[::-1]\n",
    "dysfr=dys[::-1]\n",
    "# xshr=xsh\n",
    "# yshr=ysh\n",
    "# dxshr=dxsh\n",
    "# dyshr=dysh\n",
    "fut_imgs=[]\n",
    "nusc=trainer.dl.dataset.helper.data\n",
    "for frame_id in range(fut_frames):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(40, 20))\n",
    "    anntoken=fut_tokens[frame_id]\n",
    "    ann_record = nusc.get('sample_annotation', anntoken)\n",
    "    sample_record = nusc.get('sample', ann_record['sample_token'])\n",
    "    cam = get_cam(sample_record,nusc,anntoken)\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    sd_record = nusc.get('sample_data', cam)\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    # sensor_record = nusc.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "    im = Image.open(data_path)\n",
    "\n",
    "    axes[1].imshow(im)\n",
    "    axes[1].set_title('Frame '+ str(frame_id+len(pred_imgs)+hist_frames+1) + \"    Status: Visible\", fontsize=35)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_aspect('equal')\n",
    "    def get_color(name):\n",
    "        return nusc.colormap[name]\n",
    "    assert len(boxes)==1\n",
    "    \n",
    "    for box in boxes:\n",
    "        c = np.array(get_color(box.name)) / 255.0\n",
    "        box.render(axes[1], view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "    \n",
    "    x=xsfr[frame_id]\n",
    "    y=ysfr[frame_id]\n",
    "    dx=dxsfr[frame_id]\n",
    "    dy=dysfr[frame_id]\n",
    "    axes[0].arrow(x, y, dx, dy, width=6.0, color=(1.        , 0.38823529, 0.27843137,1))\n",
    "\n",
    "    for x, y, dx, dy in zip(xsl, ysl, dxsl, dysl):\n",
    "        axes[0].arrow(x, y, dx, dy, width=1.0, color=(1,0.5,0,0.3))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].grid(False)\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    fut_imgs.append(image_from_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=hist_imgs+pred_imgs+fut_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "filename = os.path.join('./tmp', vis_sample_token,' GIF_sample' + '.gif')\n",
    "imageio.mimsave(filename, imgs, format='GIF', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = os.path.join('vis_data','no_point'+'.json')\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data_list=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}\n",
    "for token_dict in data_list:\n",
    "    key=token_dict['start']['ins_token']+\"_\"+token_dict['start']['sample_token']\n",
    "    data_dict[key]=token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=data_test['target_agent_representation']['history']\n",
    "future=data_test['target_agent_representation']['future']\n",
    "concat_motion=data_test['target_agent_representation']['concat_motion']\n",
    "time_query=data_test['target_agent_representation']['time_query']\n",
    "refine_input=data_test['target_agent_representation']['refine_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('tmp'):\n",
    "    if filename.endswith('.png'):\n",
    "        key=filename[:-4]\n",
    "        token_dict=data_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predcitions=trainer.model(data_test)\n",
    "predcitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.norm(gt_test['traj'][:,:,:2],2,dim=-1),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/home/stanliu/data/mnt/nuScenes/nuscenes', verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene = nusc.scene[0]\n",
    "first_sample_token = my_scene['first_sample_token']\n",
    "my_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "\n",
    "sensor = 'CAM_FRONT'\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "cam_front_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_annotation_token = my_sample['anns'][18]\n",
    "my_annotation_metadata =  nusc.get('sample_annotation', my_annotation_token)\n",
    "\n",
    "\n",
    "nusc.render_annotation(my_annotation_token)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('offline_trk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2303c9ae493d0f9df0ec8e1fb2be14e51470c3887efa8666f9613d9dac54ced2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
