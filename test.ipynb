{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "# from train_eval.trainer import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = torch.randn(4,15,12,2)\n",
    "traj_gt =  torch.randn(4,12,2)\n",
    "masks=torch.zeros(4,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = traj.shape[1]\n",
    "traj_gt_rpt = traj_gt.unsqueeze(1).repeat(1, num_modes, 1, 1)\n",
    "lengths = torch.sum(1-masks, dim=1).long()\n",
    "inds = lengths.unsqueeze(1).unsqueeze(2).unsqueeze(3).repeat(1, num_modes, 1, 2) - 1\n",
    "\n",
    "traj_last = torch.gather(traj[..., :2], dim=2, index=inds).squeeze(2)\n",
    "traj_gt_last = torch.gather(traj_gt_rpt, dim=2, index=inds).squeeze(2)\n",
    "\n",
    "err = traj_gt_last - traj_last[..., 0:2]\n",
    "err = torch.pow(err, exponent=2)\n",
    "err = torch.sum(err, dim=2)\n",
    "err = torch.pow(err, exponent=0.5)\n",
    "_, inds = torch.min(err, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False],\n",
       "        [False,  True,  True, False, False, False, False, False, False, False,\n",
       "          True,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False],\n",
       "        [False, False,  True, False,  True, False, False, False, False, False,\n",
       "          True,  True, False,  True,  True]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True, False, False, False,\n",
       "         False, False, False, False, False],\n",
       "        [False,  True,  True, False, False, False, False, False, False, False,\n",
       "          True,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False,  True, False, False,\n",
       "         False, False, False, False, False],\n",
       "        [False, False,  True, False,  True, False, False, False, False, False,\n",
       "          True,  True, False,  True,  True]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices= (err <= 1)\n",
    "for i in range(len(inds)):\n",
    "    # print(i,inds[i])\n",
    "    indices[i,inds[i]]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "         False, False,  True, False, False]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics.utils import min_ade,min_fde_selection\n",
    "mask_bool=min_fde_selection(traj,traj_gt,masks,1.0)\n",
    "mask_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 3.],\n",
       "         [8., 8.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.library.sampler import TorchModalitySampler\n",
    "test_sampler=TorchModalitySampler(2,1)\n",
    "endpoints,confidences = test_sampler(test)\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0434,  0.5265, -1.1014,  0.4699, -0.8214,  0.5127, -0.8310,  1.1630,\n",
       "         -0.0290,  1.1185],\n",
       "        [ 0.0434,  0.5265, -1.1014,  0.4699, -0.8214,  0.5127, -0.8310,  1.1630,\n",
       "         -0.0290,  1.1185]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_feat=torch.randn(4,8,8,10,device='cuda:0')\n",
    "endpoints=endpoints.long()\n",
    "for i in range(len(test)):\n",
    "    feat = (fake_feat[i])[endpoints[i,:,0],endpoints[i,:,1]]\n",
    "feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0325,  0.7594, -1.2122,  0.0885, -0.3049, -0.9373, -2.0800,  0.3722,\n",
       "         1.3051,  0.7324])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_feat[-1,-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel_size=3\n",
    "channel=2\n",
    "unfold = nn.Unfold(kernel_size=(kernel_size, kernel_size), dilation=1, padding=kernel_size//2, stride=(1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=unfold(test).permute(0,2,1)\n",
    "b=a.view(a.shape[0],a.shape[1],channel,kernel_size**2)\n",
    "for i,j in enumerate(a):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unfolded_feature(feature,kernel,mask):\n",
    "    ## Input shape B,C,H,W\n",
    "    channel=feature.shape[1]\n",
    "    unfold = nn.Unfold(kernel_size=(kernel, kernel), dilation=1, padding=kernel//2, stride=(1, 1))\n",
    "    unfolded_feature=unfold(feature).permute(0,2,1) ## B,Number of slided window, channel*Number of elements in every window\n",
    "    # unfolded_feature=unfolded_feature.view(unfolded_feature.shape[0],unfolded_feature.shape[1],channel,kernel**2)\n",
    "    current_node_feat=feature.view(feature.shape[0],channel,-1).permute(0,2,1)## B,Number of slided window, channel\n",
    "    # unfolded_feature=(unfolded_feature*mask.unsqueeze(-1)).to_sparse()\n",
    "    # current_node_feat=(current_node_feat*mask.unsqueeze(-1)).to_sparse()\n",
    "    target_feat=[]\n",
    "    source_feat=[]\n",
    "    for idx,batch in enumerate(current_node_feat):\n",
    "        source_feat.append(batch[mask[idx]])\n",
    "        target_feat.append(unfolded_feature[idx][mask[idx]])\n",
    "    return source_feat,target_feat\n",
    "source_feat,target_feat=get_unfolded_feature(op,15,mask_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.blocks import LayerNorm\n",
    "deocde_block=nn.Sequential(\n",
    "                nn.Linear(2*48+2,48),\n",
    "                LayerNorm(48),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(48,48//2),\n",
    "                LayerNorm(48//2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(48//2,1)\n",
    "            ).to(op.device)\n",
    "softmax=nn.Softmax(dim=-1)\n",
    "def concat_feat(source_feat,target_feat,diff,channel):\n",
    "    diff=diff.unsqueeze(0).to(source_feat[0].device)\n",
    "    connectivities=[]\n",
    "    for idx,batch in enumerate(source_feat):\n",
    "        target_batch=target_feat[idx].view(target_feat[idx].shape[0],channel,-1).permute(0,2,1)\n",
    "        batch=batch.unsqueeze(-1).repeat(1,1,target_batch.shape[-2]).permute(0,2,1)\n",
    "        concat_feat=torch.cat((batch,target_batch,diff.repeat(batch.shape[0],1,1)),dim=-1)\n",
    "        connectivity=softmax(deocde_block(concat_feat).squeeze(-1))\n",
    "        connectivities.append(connectivity)\n",
    "    return connectivities\n",
    "connectivities=concat_feat(source_feat,target_feat,diff,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord,y_coord=torch.meshgrid(torch.arange(15//2,-((15//2)+1),-1),\n",
    "                                torch.arange(-(15//2),((15//2)+1),1))\n",
    "diff=torch.cat([x_coord.unsqueeze(0),y_coord.unsqueeze(0)],dim=0)\n",
    "diff=diff.view(2,-1).T\n",
    "# diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/stanliu/code/pgp/PGP/configs/ram.yml\", 'r') as yaml_file:\n",
    "    cfg = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = cfg['dataset'] + '_' + cfg['agent_setting'] + '_' + cfg['input_representation']\n",
    "# cfg\n",
    "# cfg['encoder_args']\n",
    "# cfg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 38.729 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 13.7 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg, \"/home/stanliu/data/mnt/nuScenes/\", \"/home/stanliu/code/pgp/PGP/preprocess_raster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_eval.utils as u\n",
    "for i,data in enumerate(trainer.tr_dl):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load data\n",
    "    data = u.send_to_device(u.convert_double_to_float(data))\n",
    "    data_test=data['inputs']\n",
    "    gt_test=data['ground_truth']\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal=trainer.losses[0]\n",
    "ade=trainer.losses[1]\n",
    "# dri_loss=trainer.losses[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['ground_truth']['traj']\n",
    "map_representation = data_test['map_representation'][0]\n",
    "mask=data_test['map_representation'][1]\n",
    "\n",
    "\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "unfold = nn.Unfold(kernel_size=(7, 7), dilation=1, padding=7//2, stride=(1, 1))\n",
    "unfold(mask.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.RasterSampler import *\n",
    "sampler = Sampler(cfg['aggregator_args'],resolution=1.0,apply_mask=True)\n",
    "nodes_2D=sampler.sample_goals(mask)\n",
    "mask_under=sampler.sample_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(255*test_htmap[0,:3].permute(1,2,0)*mask)\n",
    "plt.show()\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "plt.imshow(map_representation[0].permute(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(mask.squeeze(-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "window=create_window(7, 12)\n",
    "gaussian(7, 1)\n",
    "test_heatmap=F.conv2d(test_htmap, window, padding = 7//2, groups = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data_test['surrounding_agent_representation'][4,:3].permute(1,2,0))\n",
    "plt.show()\n",
    "# plt.imshow(test_heatmap[0,0])\n",
    "# plt.show()\n",
    "# data['ground_truth']['traj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lane_node_feats', 'lane_node_masks', 's_next', 'edge_type'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['map_representation'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lane_node_feats has shape of  cuda:0\n",
      "lane_node_masks has shape of  torch.Size([6, 164, 20, 6])\n",
      "s_next has shape of  torch.Size([6, 164, 15])\n",
      "edge_type has shape of  torch.Size([6, 164, 15])\n"
     ]
    }
   ],
   "source": [
    "print('lane_node_feats has shape of ',data_test['map_representation']['lane_node_feats'].shape)\n",
    "print('lane_node_masks has shape of ',data_test['map_representation']['lane_node_masks'].shape)\n",
    "print('s_next has shape of ',data_test['map_representation']['s_next'].shape)\n",
    "print('edge_type has shape of ',data_test['map_representation']['edge_type'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         2.27%       6.699ms         2.27%       6.699ms      37.637us           0 b           0 b     481.61 Mb     481.61 Mb           178  \n",
      "                    aten::resize_         0.27%     795.788us         0.27%     795.788us      15.304us           0 b           0 b      88.06 Mb      88.06 Mb            52  \n",
      "              aten::empty_strided         0.52%       1.531ms         0.52%       1.531ms     306.272us     930.25 Kb     930.25 Kb      44.00 Mb      44.00 Mb             5  \n",
      "                         aten::to         0.05%     137.579us         1.43%       4.217ms     468.565us     930.25 Kb           0 b      44.00 Mb           0 b             9  \n",
      "                      aten::copy_         0.88%       2.615ms         0.88%       2.615ms     326.870us           0 b           0 b           0 b           0 b             8  \n",
      "                        aten::cat         0.01%      39.873us         0.41%       1.216ms     405.213us           0 b           0 b      44.03 Mb           0 b             3  \n",
      "                       aten::_cat         0.12%     353.491us         0.40%       1.176ms     391.922us           0 b           0 b      44.03 Mb           0 b             3  \n",
      "                     aten::stride         0.02%      65.218us         0.02%      65.218us       0.438us           0 b           0 b           0 b           0 b           149  \n",
      "                     aten::narrow         0.01%      21.819us         0.02%      66.697us      33.349us           0 b           0 b           0 b           0 b             2  \n",
      "                      aten::slice         0.02%      67.309us         0.03%      87.058us       8.706us           0 b           0 b           0 b           0 b            10  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 295.712ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.profiler import profile, record_function\n",
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    encodings=trainer.model.encoder(data_test)\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       aten::empty        14.71%      23.142ms        14.71%      23.142ms      91.834us           0 b           0 b       2.80 Gb       2.80 Gb           252  \n",
      "                                     aten::resize_         4.17%       6.556ms         4.17%       6.556ms      50.048us     234.47 Kb     234.47 Kb       2.70 Gb       2.70 Gb           131  \n",
      "                               aten::empty_strided         0.64%       1.004ms         0.64%       1.004ms      25.749us      58.30 Kb      58.30 Kb       1.20 Mb       1.20 Mb            39  \n",
      "                                          aten::to         0.17%     268.340us         2.08%       3.273ms      72.724us      58.30 Kb           0 b       1.19 Mb           0 b            45  \n",
      "                                  aten::as_strided         0.26%     404.348us         0.26%     404.348us       1.644us           0 b           0 b           0 b           0 b           246  \n",
      "                                      aten::repeat         0.20%     308.651us         2.80%       4.403ms     733.771us           0 b           0 b     241.03 Mb           0 b             6  \n",
      "                                      aten::expand         0.07%     115.371us         0.10%     149.972us      10.712us           0 b           0 b           0 b           0 b            14  \n",
      "                                       aten::alias         0.02%      31.384us         0.02%      31.384us       5.231us           0 b           0 b           0 b           0 b             6  \n",
      "                                      aten::unfold         0.17%     261.712us         0.24%     371.658us      19.561us           0 b           0 b           0 b           0 b            19  \n",
      "                                      aten::stride         0.10%     161.016us         0.10%     161.016us       0.482us           0 b           0 b           0 b           0 b           334  \n",
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 157.352ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    agg_feat=trainer.model.aggregator(encodings)\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "# connectivities=agg_feat['node_connectivity']\n",
    "# mask_map=agg_feat['under_sampled_mask']\n",
    "# init=agg_feat['initial_states']\n",
    "# print(connectivities.requires_grad)\n",
    "# print(mask_map.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['node_connectivity', 'under_sampled_mask', 'initial_states', 'feature', 'target_encodings'])\n",
      "torch.Size([4, 5325, 5325])\n"
     ]
    }
   ],
   "source": [
    "print(agg_feat.keys())\n",
    "print(agg_feat['node_connectivity'].shape)\n",
    "# print((agg_feat['initial_states'].indices().nelement() * agg_feat['initial_states'].indices().element_size() + agg_feat['initial_states'].values().nelement() *agg_feat['initial_states'].values().element_size()) / 1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat=op.view(op.shape[0],op.shape[1],-1).permute(0,2,1)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sp_tensor(test_feat,mask_map):\n",
    "    max_num=max(mask_map.sum(dim=1))\n",
    "    device=test_feat.device\n",
    "    attn_masks=[]\n",
    "    image_batch = []\n",
    "    init_states=[]\n",
    "    init_pos=torch.zeros([122,122],device=device)\n",
    "    init_pos[100,61]=1\n",
    "    for i,batch in enumerate(test_feat):\n",
    "        init_state=((init_pos.view(-1))[mask_map[i]]).to_sparse()\n",
    "        aug_state=torch.cat((init_state, torch.sparse_coo_tensor(torch.empty([1,0]), [], [max_num - init_state.size(0),],device=device)), 0).unsqueeze(0)\n",
    "        init_states.append(aug_state)\n",
    "        valid_nodes=batch[mask_map[i]].to_sparse(1)\n",
    "        aug_nodes=torch.cat((valid_nodes, torch.sparse_coo_tensor(torch.empty([1,0]), torch.empty([0,valid_nodes.size(1)]), [max_num - init_state.size(0),valid_nodes.size(1)],device=device)), 0).unsqueeze(0)\n",
    "        image_batch.append(aug_nodes)\n",
    "        node_mask=torch.zeros([1,max_num],device=device)\n",
    "        node_mask[:,valid_nodes.size(0):]=1\n",
    "        attn_mask=(node_mask.byte()+node_mask.byte().T)\n",
    "        attn_masks.append(attn_mask.unsqueeze(0).repeat(2,1,1))\n",
    "    return torch.cat(image_batch,dim=0).to(device),torch.cat(attn_masks,dim=0).to(device),torch.cat(init_states,dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanliu/code/pgp/PGP/models/library/sampler.py:46: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  coords = (agg == agg_max).nonzero()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     aten::resize_         6.56%       3.808ms         6.56%       3.808ms       8.120us     469.12 Kb     469.12 Kb      12.58 Mb      12.58 Mb           469  \n",
      "                               aten::empty_strided         1.04%     602.138us         1.04%     602.138us      19.424us     174.56 Kb     174.56 Kb       1.32 Mb       1.32 Mb            31  \n",
      "                                       aten::empty         3.17%       1.840ms         3.17%       1.840ms       2.504us     423.30 Kb     423.30 Kb       1.07 Mb       1.07 Mb           735  \n",
      "                                     aten::permute         0.29%     169.025us         0.32%     186.666us      62.222us           0 b           0 b           0 b           0 b             3  \n",
      "                                  aten::as_strided         2.08%       1.209ms         2.08%       1.209ms       0.770us           0 b           0 b           0 b           0 b          1569  \n",
      "                                  aten::sparse_dim         0.01%       8.677us         0.01%       8.677us       1.446us           0 b           0 b           0 b           0 b             6  \n",
      "                                   aten::dense_dim         0.00%       1.357us         0.00%       1.357us       1.357us           0 b           0 b           0 b           0 b             1  \n",
      "                                    aten::_indices         0.01%       4.773us         0.01%       4.773us       0.796us           0 b           0 b           0 b           0 b             6  \n",
      "                                      aten::narrow         0.03%      19.716us         0.09%      50.056us       8.343us           0 b           0 b           0 b           0 b             6  \n",
      "                                       aten::slice         2.38%       1.380ms         2.72%       1.577ms       6.827us           0 b           0 b           0 b           0 b           231  \n",
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 58.078ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    predictions=trainer.model.decoder(agg_feat)\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "# print(predictions['pred'].element_size() * predictions['pred'].nelement()/ 1024**2)\n",
    "# pred=predictions['pred'].to_sparse()\n",
    "# print(pred.element_size() * pred.nelement()/ 1024**2)\n",
    "\n",
    "# print((pred.indices().nelement() * pred.indices().element_size() + pred.values().nelement() * pred.values().element_size()) / 1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = agg_feat\n",
    "attn_output_weights = inputs['node_connectivity']\n",
    "init_states=inputs['initial_states']\n",
    "\n",
    "\n",
    "mask=inputs['under_sampled_mask']\n",
    "target_encodings = inputs['target_encodings']\n",
    "map_feature=inputs['feature'].permute(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "predictions=torch.empty([init_states.shape[0],0,init_states.shape[-1]],device=attn_output_weights.device)\n",
    "prev_states=init_states.unsqueeze(1).to_dense()\n",
    "\n",
    "for step in range(12):\n",
    "    predictions=torch.cat((predictions,torch.bmm(prev_states,attn_output_weights)),dim=1)\n",
    "    prev_states=predictions[:,step].unsqueeze(1)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_extent= [-61, 61, -22, 100]\n",
    "from models.decoders.ram_decoder import get_dense,get_index\n",
    "nodes_2D=get_index(predictions[:,-1].unsqueeze(1),mask)\n",
    "dense_pred=get_dense(predictions[:,-1].unsqueeze(1),nodes_2D,122,122)\n",
    "endpoints,confidences = trainer.model.decoder.endpoint_sampler(dense_pred)\n",
    "endpoints=endpoints.long()\n",
    "concat_feature=torch.empty([0,12,90],device=attn_output_weights.device)\n",
    "x_coord,y_coord=torch.meshgrid(torch.arange(map_extent[-1],map_extent[-2],-1), ##### SHould be changed when image size changes\n",
    "                                torch.arange(map_extent[0],map_extent[1],1))\n",
    "indices=torch.cat([x_coord.unsqueeze(-1),y_coord.unsqueeze(-1)],dim=-1).to(attn_output_weights.device)\n",
    "for batch_idx in range(len(dense_pred)):\n",
    "    map_feat = (map_feature[batch_idx])[endpoints[batch_idx,:,0],endpoints[batch_idx,:,1]]\n",
    "    diff = indices[endpoints[batch_idx,:,0],endpoints[batch_idx,:,1]].float()\n",
    "\n",
    "    feature=torch.cat([map_feat,trainer.model.decoder.diff_encoder(diff),target_encodings[batch_idx].repeat(trainer.model.decoder.endpoint_sampler._n_targets,1)],dim=-1).unsqueeze(0)\n",
    "    concat_feature=torch.cat([concat_feature,feature],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predictions['pred']\n",
    "mask=predictions['mask']\n",
    "traj_gt = gt_test['traj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    swapped=torch.zeros_like(traj_gt).to(pred.device)\n",
    "    swapped[:,:,0],swapped[:,:,1]=-traj_gt[:,:,1],traj_gt[:,:,0]\n",
    "    coord=torch.round(swapped/trainer.losses[0].resolution+trainer.losses[0].compensation).int()\n",
    "    coord=torch.clamp(coord,0,122)\n",
    "    gt_map=torch.zeros([6,12,122,122],device=pred.device)\n",
    "    for batch in range(6):\n",
    "        for t in range(12):\n",
    "            x,y=coord[batch,t]\n",
    "            gt_map[batch,t,x,y]=1##Only one ground truth in each heatmap layer\n",
    "    gs_map=F.conv2d(gt_map, trainer.losses[0].window, padding = trainer.losses[0].window_size//2, groups = trainer.losses[0].horizon)\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    loss_mask = (reduced_gts == 1).float()\n",
    "    pred_heatmap = torch.clamp(pred, min=1e-4)\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    loss=-torch.sum(\n",
    "                    torch.pow(pred_heatmap - reduced_maps, 2) * (\n",
    "                    loss_mask * torch.log(pred_heatmap)\n",
    "                    +\n",
    "                    (1-loss_mask) * (torch.pow(1 - reduced_maps, 4) * torch.log(1 - pred_heatmap))\n",
    "                )\n",
    "            )\n",
    "    loss.backward()\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    trainer.optimizer.step()\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "    gs_map=gs_map.view([6,12,-1])\n",
    "    gt_map=gt_map.view([6,12,-1])\n",
    "    max_num=max(mask.sum(dim=1))\n",
    "    reduced_maps=[]\n",
    "    reduced_gts=[]\n",
    "    for i,batch in enumerate(gs_map):\n",
    "        reduced_map=batch[mask[i].repeat(12,1)].view(12,-1)\n",
    "        reduced_gt=gt_map[i][mask[i].repeat(12,1)].view(12,-1)\n",
    "        aug_map=torch.cat((reduced_map, torch.zeros(12,max_num - reduced_map.size(1),device=pred.device)), -1)\n",
    "        aug_gt=torch.cat((reduced_gt, torch.zeros(12,max_num - reduced_map.size(1),device=pred.device)), -1)\n",
    "        reduced_maps.append(aug_map.unsqueeze(0))\n",
    "        reduced_gts.append(aug_gt.unsqueeze(0))\n",
    "    reduced_maps=torch.cat(reduced_maps,dim=0).to(pred.device)\n",
    "    reduced_gts=torch.cat(reduced_gts,dim=0).to(pred.device)\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72., device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_gts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(pred,mask):\n",
    "    x_coord,y_coord=torch.meshgrid(torch.arange(0,122,1),\n",
    "                                torch.arange(0,122,1))\n",
    "    nodes_candidates=torch.cat((x_coord.unsqueeze(0),y_coord.unsqueeze(0)),dim=0).view(2,-1).T\n",
    "    nodes_2D=torch.zeros([mask.shape[0],pred.shape[-1],2])\n",
    "    for i in range(mask.shape[0]):\n",
    "        nodes_batch=nodes_candidates[mask[i]]\n",
    "        nodes_2D[i,:nodes_batch.shape[0]]=nodes_batch\n",
    "    return nodes_2D.int().permute(0,2,1).to(pred.device)\n",
    "nodes_2D=get_index(pred,mask)\n",
    "nodes_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(pred,nodes_2D,H,W):\n",
    "    dense_rep=torch.empty(0,pred.shape[1],H,W,device=pred.device)\n",
    "    for batch in range(pred.shape[0]):\n",
    "        batch_heatmap=torch.empty(0,H,W,device=pred.device)\n",
    "        for step in range(pred.shape[1]):\n",
    "            heatmap=torch.sparse_coo_tensor(nodes_2D[batch],pred[batch,step],(122,122))\n",
    "            batch_heatmap=torch.cat((batch_heatmap,heatmap.to_dense().unsqueeze(0)),dim=0)\n",
    "        dense_rep=torch.cat((dense_rep,batch_heatmap.unsqueeze(0)),dim=0)\n",
    "    return dense_rep\n",
    "dense_rep=get_dense(pred,nodes_2D,122,122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['probs']=predictions['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 aten::empty         1.20%      26.132ms         1.20%      26.132ms      42.560us         512 b         512 b      12.70 Gb      12.70 Gb           614  \n",
      "                               aten::resize_         0.44%       9.528ms         0.44%       9.528ms      54.756us         512 b         512 b       7.02 Gb       7.02 Gb           174  \n",
      "                         aten::empty_strided         0.08%       1.769ms         0.08%       1.769ms      15.122us           0 b           0 b       1.47 Gb       1.47 Gb           117  \n",
      "         aten::thnn_conv_depthwise2d_forward         0.00%      84.483us         0.00%      84.483us      84.483us           0 b           0 b       2.73 Mb       2.73 Mb             1  \n",
      "                            aten::zeros_like         0.01%     178.300us         0.04%     939.082us     469.541us           0 b           0 b      58.14 Mb           0 b             2  \n",
      "                            aten::empty_like         0.02%     341.439us         0.10%       2.273ms      44.560us           0 b           0 b       2.14 Gb           0 b            51  \n",
      "                                 aten::zero_         0.02%     507.300us         0.11%       2.492ms      35.096us           0 b           0 b           0 b           0 b            71  \n",
      "                                 aten::fill_         0.13%       2.837ms         0.13%       2.837ms      23.644us           0 b           0 b           0 b           0 b           120  \n",
      "                                    aten::to         0.01%     201.428us         0.04%     841.042us      13.350us           0 b           0 b       3.03 Mb           0 b            63  \n",
      "                                 aten::slice         0.07%       1.484ms         0.08%       1.699ms      13.069us           0 b           0 b           0 b           0 b           130  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.170s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with profile( profile_memory=True, record_shapes=True) as prof:\n",
    "\n",
    "# loss=pred.sum()\n",
    "    loss=focal.compute(predictions,gt_test)\n",
    "    loss+=ade.compute(predictions,gt_test)\n",
    "    loss.backward()\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions['pred']\n",
    "mask_da = predictions['mask'].view(-1,pred.shape[-2],pred.shape[-1]).unsqueeze(1)\n",
    "ground_truth = gt_test\n",
    "traj_gt = ground_truth['traj'] if type(ground_truth) == dict else ground_truth\n",
    "true_heatmap,gs_map = focal.generate_gtmap(traj_gt,pred.shape)\n",
    "gs_map=gs_map*mask_da\n",
    "mask = (true_heatmap == 1).float()\n",
    "pred_heatmap = torch.clamp(pred, min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= focal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions['pred']\n",
    "mask = predictions['mask'].view(-1,pred.shape[-2],pred.shape[-1]).unsqueeze(1)\n",
    "non_drivable_area_mask=~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(non_drivable_area_mask[0,0].cpu())\n",
    "plt.show()\n",
    "plt.imshow(torch.zeros_like(non_drivable_area_mask[0,0].cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "map_extent=[ -50, 50, -20, 80 ]\n",
    "img_size=[400,400]\n",
    "resolution = (map_extent[1] - map_extent[0]) /  img_size[1]\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=\"/home/stanliu/data/mnt/nuScenes/\", verbose=True)\n",
    "helper=PredictHelper(nusc)\n",
    "map_rasterizer = StaticLayerRasterizer(helper,\n",
    "                                        resolution=resolution,\n",
    "                                        meters_ahead=map_extent[3],\n",
    "                                        meters_behind=-map_extent[2],\n",
    "                                        meters_left=-map_extent[0],\n",
    "                                        meters_right=map_extent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "target_agent_representation = data_test['target_agent_representation']\n",
    "surrounding_agent_representation = data_test['surrounding_agent_representation']\n",
    "map_representation = data_test['map_representation'][0]\n",
    "mask= data_test['map_representation'][1].type(torch.bool)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.array(mask[idx]))\n",
    "plt.show()\n",
    "plt.imshow(np.array(map_representation[idx]).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models import resnet34\n",
    "# input = torch.cat((map_representation, surrounding_agent_representation), dim=1)\n",
    "# resnet_model = resnet34(pretrained=False)\n",
    "# conv1_new = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# modules = list(resnet_model.children())[:-2]\n",
    "\n",
    "# modules[0] = conv1_new\n",
    "# backbone = nn.Sequential(*modules)\n",
    "# data_test['target_agent_representation'].float().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoders.raster_encoder import *\n",
    "encoder=RasterEncoder(cfg['encoder_args'])\n",
    "# encodings=encoder.forward(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings['context_encoding'][\"combined\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.modules as nn\n",
    "fake_map_encodings=torch.randn(32, 512, 16,16)\n",
    "fake_agent_input=torch.randn(32, 32).unsqueeze(2).unsqueeze(3).repeat(1,1,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_encodings=torch.cat([fake_map_encodings,fake_agent_input],dim=1)\n",
    "conv1d1=nn.Conv2d(544, 528, kernel_size=1, stride=1, bias=False)\n",
    "conv1d2=nn.Conv2d(528, 512, kernel_size=1, stride=1, bias=False)\n",
    "test_dim_reduction=nn.Sequential(conv1d1,nn.BatchNorm2d(528),nn.ReLU(),conv1d2,nn.BatchNorm2d(512),nn.ReLU())\n",
    "fake_feature=test_dim_reduction(concatenated_encodings)\n",
    "fake_feature=fake_feature.view(fake_feature.shape[0], fake_feature.shape[1], -1).permute(0, 2, 1)\n",
    "\n",
    "fake_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_test=final_convs(transpose_convs(fake_feature))\n",
    "# augmented_mask=mask.unsqueeze(-1)\n",
    "print(upsampled_test[:,:,::2,::2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from model.decode import generic_decode\n",
    "\n",
    "# from utils.image import gaussian_radius, draw_umich_gaussian\n",
    "\n",
    "# from model.ConvGRU import ConvGRU\n",
    "\n",
    "from einops import rearrange as rearr, repeat\n",
    "\n",
    "from spatial_correlation_sampler import spatial_correlation_sample\n",
    "\n",
    "\n",
    "class LocalWalk(nn.Module):\n",
    "    def __init__(self, topk=0, radius=0.05, temp=0.05, pad_value=0,\n",
    "            broadcast_val=False, corr_module=True):\n",
    "        super(LocalWalk, self).__init__()\n",
    "\n",
    "        self.topk = topk\n",
    "        self.radius = radius\n",
    "        self.vals = {}\n",
    "        self.idxmaps = {}\n",
    "        self.temp = temp\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.broadcast_val = broadcast_val\n",
    "\n",
    "        self.corr_module = corr_module\n",
    "\n",
    "    def get_identity_label(self, keys):\n",
    "        '''\n",
    "        returns 1 x H*W x H x W as reshaped H*W x H*W identity matrix\n",
    "        '''\n",
    "        B, C, H, W = keys.shape\n",
    "        name = f\"{H}_{W}\"\n",
    "        if name not in self.vals:\n",
    "            vals = self.distance_field(H, W).flatten(0, 1)\n",
    "            vals = (vals == 0).float() ##Returns an identity matrix, which is composed of multiple matrices.\n",
    "            # The i th matrix has an element 1 at the i ith position, the rest places are all zeros.\n",
    "            vals = repeat(vals, 'n h w -> b n h w', b=B if not self.broadcast_val else 1)## Repeat for batch number times\n",
    "            self.vals[name] = vals.to(keys.device)\n",
    "            print('created vals')\n",
    "\n",
    "        return self.vals[name]\n",
    "\n",
    "    def knn(self, A):\n",
    "        if self.pad_value == 0 or self.topk > 0:\n",
    "            with torch.no_grad():\n",
    "                mask = (A == self.pad_value)\n",
    "                if self.topk > 0:\n",
    "                  mask |= (A < A.topk(k=self.topk, dim=-1)[0].min(-1, keepdim=True)[0])\n",
    "            A[mask] = -10\n",
    "\n",
    "        return A\n",
    "\n",
    "    def distance_field(self, H, W, p=2):\n",
    "        gx, gy = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))\n",
    "        D = ( (gx[None, None, :, :] - gx[:, :, None, None]).abs()**p + (gy[None, None, :, :] - gy[:, :, None, None]).abs()**p ).float() #** (1/p)\n",
    "        return D\n",
    "\n",
    "    def make_scatter_map(self, keys, kH, kW):\n",
    "        B, C, H, W = keys.shape\n",
    "        name = f\"{H}_{W}_{kH}_{kW}\"\n",
    "        if name not in self.idxmaps:\n",
    "            idx_map = torch.arange(H*W).view(H, W)[None, None].float()\n",
    "            idx_map = torch.nn.functional.unfold(idx_map, kernel_size=(kH, kW), stride=1, padding=(kH//2, kW//2))\n",
    "            idx_map = rearr(idx_map, 'b n hw -> b hw n')\n",
    "            idx_map = idx_map.clamp(min=0).long()\n",
    "            self.idxmaps[name] = idx_map.to(keys.device)\n",
    "            print('created idx map')\n",
    "\n",
    "        return self.idxmaps[name]\n",
    "\n",
    "    def forward(self, query, keys, val=None):\n",
    "        '''\n",
    "        assumes q, k, v: B D N\n",
    "        '''\n",
    "\n",
    "        B, C, H, W = keys.shape\n",
    "        kW = kH = int(H * self.radius) * 2 + 1\n",
    "\n",
    "        val = self.make_scatter_map(keys, kH, kW)## Returns the indices of elements inside the sliding windows of all steps\n",
    "        ## The sliding window has size (kH,kW), the input is keys.\n",
    "        # out = self.get_identity_label(keys) * 0##Why all zeros?\n",
    "        out = torch.zeros([B,H*W,H,W]).float()\n",
    "        out = repeat(out, '1 n h w -> b (h w) n', b=B) if out.shape[0] == 1 else \\\n",
    "              rearr(out, 'b n h w -> b (h w) n') ##Repeat for batch number\n",
    "\n",
    "        if self.corr_module:\n",
    "            att = spatial_correlation_sample(query,\n",
    "                               keys,\n",
    "                               kernel_size=1,\n",
    "                               patch_size=kH,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               dilation=1,\n",
    "                               dilation_patch=1) / self.temp\n",
    "            att = rearr(att, 'b p1 p2 h w -> b h w (p1 p2)')##Local connectivity, for each node (pixel)\n",
    "            ## calculate its node similarity with nearby nodes\n",
    "\n",
    "        A = self.knn(att)\n",
    "\n",
    "        A = torch.exp(rearr(A, 'b h w n -> b (h w) n'))\n",
    "        out.scatter_add_(2, val.to(A.device).expand_as(A), A)\n",
    "        val = rearr(out, 'b (h w) n -> b n h w', h=H)\n",
    "\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor=torch.randn([32,20,100,100])\n",
    "test_walker=LocalWalk(radius=0.2)\n",
    "test_walker.forward(test_tensor,test_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.RasterSampler import *\n",
    "sampler=Sampler(cfg['train_set_args'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mask=torch.randn([32,488,488]).ge(0)\n",
    "nodes_2D=sampler.sample_goals().repeat(32,1,1).type(torch.float32)\n",
    "mask_under=(sampler.sample_mask(mask))\n",
    "attn_mask=~mask_under.unsqueeze(-1).repeat(2,1,256)\n",
    "print(attn_mask.shape)\n",
    "print(mask.shape)\n",
    "print(nodes_2D.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.array(mask[0]))\n",
    "plt.show()\n",
    "plt.imshow(np.array(~mask_under[0].view(122,122)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/stanliu/code/pgp/PGP/configs/preprocess_nuscenes.yml\", 'r') as yaml_file:\n",
    "    cfg = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 49.698 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 14.8 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from datasets.nuScenes.nuScenes_graphs import NuScenesGraphs\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=\"/home/stanliu/data/mnt/nuScenes/\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.prediction import PredictHelper\n",
    "helper=PredictHelper(nusc)\n",
    "graph_extractor=NuScenesGraphs('extract_data','/home/stanliu/code/pgp/PGP/preprocess',cfg['train_set_args'],helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_extractor.max_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "i_t, s_t = graph_extractor.token_list[idx].split(\"_\")\n",
    "map_name = graph_extractor.helper.get_map_name_from_sample_token(s_t)\n",
    "map_api = graph_extractor.maps[map_name]\n",
    "\n",
    "# Get agent representation in global co-ordinates\n",
    "global_pose = graph_extractor.get_target_agent_global_pose(idx)\n",
    "\n",
    "# Get lanes around agent within map_extent\n",
    "lanes = graph_extractor.get_lanes_around_agent(global_pose, map_api)\n",
    "\n",
    "# Get relevant polygon layers from the map_api\n",
    "polygons = graph_extractor.get_polygons_around_agent(global_pose, map_api)\n",
    "\n",
    "# Get vectorized representation of lanes\n",
    "lane_node_feats, lane_ids = graph_extractor.get_lane_node_feats(global_pose, lanes, polygons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_line': [<shapely.geometry.polygon.Polygon at 0x7fc6455b7a90>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb1f3c6aa90>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3850>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3b90>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3ed0>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3650>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3610>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc37d0>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc36d0>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc39d0>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3c50>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3b10>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3510>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3950>],\n",
       " 'ped_crossing': [<shapely.geometry.polygon.Polygon at 0x7fb184dc3d50>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb184dc3750>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fc5bc7539d0>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fc46b1db590>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fc5bc753050>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fc5bc753510>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb1f3c6fd10>,\n",
       "  <shapely.geometry.polygon.Polygon at 0x7fb1f3c6f950>]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_node_feats, lane_ids = graph_extractor.discard_poses_outside_extent(lane_node_feats, lane_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_succ = graph_extractor.get_successor_edges(lane_ids, map_api)\n",
    "e_prox = graph_extractor.get_proximal_edges(lane_node_feats, e_succ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_nbrs = [len(e_succ[i]) + len(e_prox[i]) for i in range(len(e_succ))]\n",
    "max_nbrs = max(num_nbrs) if len(num_nbrs) > 0 else 0\n",
    "num_nodes = len(lane_node_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nbrs [3, 4, 2, 1, 4, 4, 1, 0, 5, 1, 2, 5, 3, 3, 2, 1, 1, 7, 7, 4, 1, 1, 3, 1, 1, 3, 3, 5, 1, 1, 5, 2, 3, 3, 6, 2, 1, 4, 3, 2, 4, 6, 3, 2, 6, 6, 4, 4, 3, 2, 3, 0, 1, 3, 3, 5, 7, 0, 2, 2, 4, 2, 3, 6, 6, 2]\n",
      "max_nbrs 7\n",
      "num_nodes 66\n"
     ]
    }
   ],
   "source": [
    "print('num_nbrs',(num_nbrs))\n",
    "print('max_nbrs',max_nbrs)\n",
    "print('num_nodes',num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_next, edge_type = graph_extractor.get_edge_lookup(e_succ, e_prox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.802461953834806, 3.061186343714443, 7.854522040816855, 7.216796645669941, 18.576843035250178, 17.59911878742139, 10.914208236956977, 9.068873933462351, 11.762230834468642, 13.572448625068171, 12.602988688139565, 9.425613872688336, 9.402447894083195, 7.5050484915162485, 8.871170865969505, 14.103420189579426, 12.247852570683502, 16.841649744374237, 16.841650182474936, 5.206380624297952, 13.998243967231723, 12.998369441423108, 4.300186415504187, 9.640125442146143, 8.676119749606176, 13.96183763756843, 12.964563305554275, 10.872754846318436, 11.125382861420317, 10.760536504026037, 8.046168305085073, 18.711690348211377, 10.32347042088223, 11.061311388227276, 10.182122498686464, 15.779823573174108, 9.514357587541612, 8.628636263351288, 10.362995112937323, 9.39499609484185, 10.426028772133298, 8.647760162693757, 10.106761792612282, 13.217534541937985, 13.665098919423087, 12.928435912813006, 11.00013312310872, 15.0002675029557, 14.319079389966237, 10.623236994898685, 10.146254732155962, 16.541618056115425, 9.948729063735433, 9.67379751537185, 9.673797515371803, 11.96578651293105, 18.527657608467422, 11.676575223277476, 18.12640094531441, 9.950194586392799, 10.386761005281198, 13.244855726530513, 12.625758624805846, 16.291136269347774, 11.92558336070148, 9.456910396375136]\n",
      "[0.967080537071126, 0.7652962098865776, 0.9818152018461492, 0.9020991901482234, 0.9777284693513656, 0.9777288215234142, 0.9922008992304064, 0.9921992998980314, 0.9801853061327938, 0.9694606122659606, 0.9694606683184014, 0.9425616655840595, 0.9402447083055451, 0.938130614437005, 0.985685421316696, 0.940229246802806, 2.54653314751042e-13, 0.9906852790807248, 0.9906852790808034, 0.8677300912913167, 0.9998745690879703, 0.9998746123674964, 0.8600372819767556, 0.9640117749933822, 0.9640133055118817, 0.9972740815506563, 0.9972741228774249, 0.9884322622378723, 0.9271152088800749, 0.9785892534352242, 0.9780354076775306, 0.984825509827708, 0.9384973115309145, 0.9217758831057566, 0.9392243773488917, 0.9862389541064929, 0.9515451441837471, 0.9513889480661701, 0.9420902249036701, 0.9420902249036981, 0.9478207653117954, 0.9478207653118097, 0.9187965604271522, 0.9439814376892603, 0.9760785281172628, 0.9944950427075843, 0.9944950427075919, 0.9375140097018182, 0.9546082563478423, 0.9657710105990798, 0.9230615644032493, 0.973036356242143, 0.9948729063735647, 0.9673797515371366, 0.9673797515371848, 0.9969413124575012, 0.9751398579272266, 0.9730469533240259, 0.9539762032293061, 0.995019464215615, 0.9442433588641813, 0.9460611233236192, 0.9461935334422396, 0.958301892937787, 0.9942282285726826, 0.9937440700871167]\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "lengths=[]\n",
    "segment_lengths=[]\n",
    "for idx in range(len(lane_node_feats)):\n",
    "    lengths.append(LA.norm(lane_node_feats[idx][1:,:2]-lane_node_feats[idx][:-1,:2],axis=1).sum())\n",
    "    segment_lengths.append(LA.norm(lane_node_feats[idx][1:,:2]-lane_node_feats[idx][:-1,:2],axis=1)[0])\n",
    "print(lengths)\n",
    "print(segment_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8017b5dcd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "prev_feat=np.empty([0,5])\n",
    "for feat in lane_node_feats[:2]:\n",
    "    prev_feat=np.r_[prev_feat,feat]\n",
    "idx=-10\n",
    "import matplotlib.pyplot as plt\n",
    "x=prev_feat[:,0]\n",
    "y=prev_feat[:,1]\n",
    "plt.scatter(x,y,s=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pgp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "593542b65644041049b318720dfeebe70f9074a000f082b969de259755a1a643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
