{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "# from train_eval.trainer import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/occlusion_train_v2_vis.yml\", 'r') as yaml_file:\n",
    "    cfg = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "34149 lidarseg,\n",
      "Done loading in 53.068 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 10.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from train_eval.evaluator import Evaluator\n",
    "trainer = Evaluator(cfg, \"/home/stanliu/data/mnt/nuScenes/nuscenes\", \"vis_data\",\"track_completion_model/track_completion_att2.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_eval.utils as u\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(trainer.dl):\n",
    "        # torch.cuda.empty_cache()\n",
    "        # Load data\n",
    "        data = u.send_to_device(u.convert_double_to_float(data))\n",
    "        data_test=data['inputs']\n",
    "        gt_test=data['ground_truth']\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['instance_token', 'sample_token', 'map_representation', 'target_agent_representation', 'origin'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "from typing import Dict, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets.nuScenes.prediction import PredictHelper_occ\n",
    "from nuscenes.prediction.input_representation.static_layers import *\n",
    "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.eval.common.utils import quaternion_yaw\n",
    "import logging\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "from PIL import Image\n",
    "import copy\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from executables.track_completion import *\n",
    "def get_cam(sample_record,nusc,anntoken):\n",
    "    cams = [key for key in sample_record['data'].keys() if 'CAM' in key]\n",
    "    for cam in cams:\n",
    "        _, boxes, _ = nusc.get_sample_data(sample_record['data'][cam], box_vis_level=BoxVisibility.ANY,\n",
    "                                                selected_anntokens=[anntoken])\n",
    "        if len(boxes) > 0:\n",
    "            break  # We found an image that matches. Let's abort.\n",
    "    assert len(boxes) > 0, 'Error: Could not find image where annotation is visible. ' \\\n",
    "                            'Try using e.g. BoxVisibility.ANY.'\n",
    "    assert len(boxes) < 2, 'Error: Found multiple annotations. Something is wrong!'\n",
    "\n",
    "    cam = sample_record['data'][cam]\n",
    "    return cam\n",
    "def render_occ_anns(frame,nusc: NuScenes,global_coord,global_rotation,save_dir,key,frame_id,save):\n",
    "    anntoken=frame['ann_token']\n",
    "    ann_record = nusc.get('sample_annotation', anntoken)\n",
    "    sample_record = nusc.get('sample', ann_record['sample_token'])\n",
    "    cam = get_cam(sample_record,nusc,anntoken)\n",
    "    z=nusc.get('sample_annotation', anntoken)['translation'][-1]\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])\n",
    "    sd_record = nusc.get('sample_data', cam)\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    # sensor_record = nusc.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "\n",
    "    im = Image.open(data_path)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(9, 9))\n",
    "    axes.imshow(im)\n",
    "    axes.set_title(nusc.get('sample_data', cam)['channel'])\n",
    "    axes.axis('off')\n",
    "    axes.set_aspect('equal')\n",
    "    def get_color(name):\n",
    "        return nusc.colormap[name]\n",
    "    assert len(boxes)==1\n",
    "    pred_box=copy.deepcopy(boxes[0])\n",
    "    pred_box.name=\"human.pedestrian.adult\"\n",
    "    pred_box.center=global_coord+[z]\n",
    "    pred_box.orientation=Quaternion(global_rotation)\n",
    "\n",
    "    pred_box.translate(-np.array(pose_record['translation']))\n",
    "    pred_box.rotate(Quaternion(pose_record['rotation']).inverse)\n",
    "\n",
    "    #  Move box to sensor coord system.\n",
    "    pred_box.translate(-np.array(cs_record['translation']))\n",
    "    pred_box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "    boxes.append(pred_box)\n",
    "    # boxes.append()\n",
    "    for box in boxes:\n",
    "        c = np.array(get_color(box.name)) / 255.0\n",
    "        box.render(axes, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "    if save:\n",
    "        out_dir=os.path.join(save_dir,key)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path=os.path.join(out_dir,str(frame_id))\n",
    "        plt.savefig(out_path)\n",
    "        plt.close(fig)\n",
    "    return\n",
    "\n",
    "@torch.no_grad()\n",
    "def local_pose_to_image(local_poses,pose_mask,resolution,img_size,arror_length=None):\n",
    "    '''local_poses: [T,4] \n",
    "    mask: [T]\n",
    "    '''\n",
    "    if arror_length is None:\n",
    "        arror_length=8\n",
    "    y_m=np.asarray(local_poses[:,1][pose_mask].cpu())\n",
    "    x_m=np.asarray(local_poses[:,0][pose_mask].cpu())\n",
    "    img_origin=np.round(np.asarray(img_size)/2).astype(np.int)\n",
    "    x=img_origin[1]+x_m*resolution\n",
    "    y=img_origin[0]-y_m*resolution\n",
    "    yaw=np.asarray((local_poses[:,2][pose_mask]).cpu())\n",
    "    dy=-np.sin(yaw+np.pi/2)*arror_length\n",
    "    dx=np.cos(yaw+np.pi/2)*arror_length\n",
    "    return x,y,dx,dy\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(inputs: Dict,ground_truth: Dict,predictions: Dict,helper: PredictHelper_occ, selector,token_dicts,save_folder='./tmp', mode='refine',save=False):\n",
    "\n",
    "    upper_limit=100\n",
    "    batch_size=len(predictions['traj'])\n",
    "    layer_names = ['drivable_area', 'ped_crossing']\n",
    "    maps= load_all_maps(helper)\n",
    "    colors = [(255, 255, 255), (119, 136, 153)]\n",
    "    for sample_id in range(batch_size):\n",
    "        if sample_id>upper_limit:\n",
    "            return\n",
    "        # try:\n",
    "        if not selector[sample_id]:\n",
    "            continue\n",
    "        instance_token=inputs['instance_token'][sample_id]\n",
    "        sample_token=inputs['sample_token'][sample_id]\n",
    "        future=inputs['target_agent_representation']['future']['traj'][sample_id]\n",
    "        mask_fut=inputs['target_agent_representation']['future']['mask'][sample_id]\n",
    "        hist=inputs['target_agent_representation']['history']['traj'][sample_id]\n",
    "        nearest_idx=np.where(mask_fut[:, 0].cpu() == 0)[0][-1]\n",
    "        prediction_horizon=future[nearest_idx,-1]\n",
    "        sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "            \n",
    "        map_name = helper.get_map_name_from_sample_token(sample_token)\n",
    "        x, y = sample_annotation['translation'][:2]\n",
    "        yaw = quaternion_yaw(Quaternion(sample_annotation['rotation']))\n",
    "        yaw_corrected = correct_yaw(yaw)\n",
    "        global_pose=(x,y,yaw_corrected)\n",
    "        if 'origin' in inputs:\n",
    "            origin=tuple([inputs['origin'][sample_id,0].item(),inputs['origin'][sample_id,1].item(),inputs['origin'][sample_id,2].item()])\n",
    "        else:\n",
    "            coords_fut,global_yaw_fut,time_fut = helper.get_future_for_agent(instance_token, sample_token, seconds=2+prediction_horizon, in_agent_frame=False,add_yaw_and_time=True)\n",
    "\n",
    "            sep_idx= np.searchsorted(time_fut, (prediction_horizon-0.001).item())\n",
    "            origin_fut=coords_fut[sep_idx][0],coords_fut[sep_idx][1],correct_yaw(quaternion_yaw(Quaternion(global_yaw_fut[sep_idx])))\n",
    "            origin=tuple((np.asarray(global_pose)+np.asarray(origin_fut))/2)\n",
    "        dist=LA.norm(future[0,:2].cpu(),ord=2)\n",
    "        image_side_length = 2 * max(25,dist+10)\n",
    "        image_side_length_pixels = 400\n",
    "        resolution=image_side_length_pixels/image_side_length\n",
    "        patchbox = get_patchbox(origin[0], origin[1], image_side_length)\n",
    "\n",
    "        angle_in_degrees = angle_of_rotation(origin[2]) * 180 / np.pi\n",
    "\n",
    "        canvas_size = (image_side_length_pixels, image_side_length_pixels)\n",
    "        masks = maps[map_name].get_map_mask(patchbox, angle_in_degrees, layer_names, canvas_size=canvas_size)\n",
    "        \n",
    "        images = []\n",
    "        for mask, color in zip(masks, colors):\n",
    "            images.append(change_color_of_binary_mask(np.repeat(mask[::-1, :, np.newaxis], 3, 2), color))\n",
    "        if mode=='refine':\n",
    "            traj = predictions['refined_traj'][sample_id].squeeze(0)\n",
    "            yaw = predictions['refined_yaw'][sample_id]\n",
    "        elif mode=='raw':\n",
    "            traj = predictions['traj'][sample_id].squeeze(0)\n",
    "            yaw = predictions['yaw'][sample_id]\n",
    "        lanes=inputs['map_representation']['lane_node_feats'][sample_id].flatten(0,1).clone()\n",
    "        lanes_mask=inputs['map_representation']['lane_node_feats'][sample_id].flatten(0,1)[:,0].bool()\n",
    "        pred = torch.cat((traj,yaw),-1)\n",
    "        pose_pred_mask=~(predictions['mask'][sample_id]).bool()\n",
    "        gt = ground_truth['traj'][sample_id]\n",
    "        image = Rasterizer().combine(images)\n",
    "        pose_future_mask=~inputs['target_agent_representation']['future']['mask'][sample_id][:,0].bool()\n",
    "        pose_hist_mask=~inputs['target_agent_representation']['history']['mask'][sample_id][:,0].bool()\n",
    "        xs, ys, dxs, dys=local_pose_to_image(future,pose_future_mask,resolution,canvas_size)\n",
    "        xsh, ysh, dxsh, dysh=local_pose_to_image(hist,pose_hist_mask,resolution,canvas_size)\n",
    "        xsp, ysp, dxsp, dysp=local_pose_to_image(pred,pose_pred_mask,resolution,canvas_size,5)\n",
    "        xsg, ysg, dxsg, dysg=local_pose_to_image(gt,pose_pred_mask,resolution,canvas_size,5)\n",
    "        xsl, ysl, dxsl, dysl=local_pose_to_image(lanes,lanes_mask,resolution,canvas_size,2.5)\n",
    "        # plt.imshow(image)\n",
    "        fig = plt.figure(figsize = (8,8))\n",
    "        ax = fig.add_subplot(1,1,1) \n",
    "        \n",
    "        for x, y, dx, dy in zip(xs, ys, dxs, dys):\n",
    "            ax.arrow(x, y, dx, dy, width=0.8, color=(1,0,0,1))\n",
    "        for x, y, dx, dy in zip(xsh, ysh, dxsh, dysh):\n",
    "            ax.arrow(x, y, dx, dy, width=0.8, color=(0,1,0,1))\n",
    "        for x, y, dx, dy in zip(xsp, ysp, dxsp, dysp):\n",
    "            ax.arrow(x, y, dx, dy, width=1.0, color=(0.0,0,1,1))\n",
    "        for x, y, dx, dy in zip(xsg, ysg, dxsg, dysg):\n",
    "            ax.arrow(x, y, dx, dy, width=1.0, color=(1,0,1,0.3))\n",
    "        for x, y, dx, dy in zip(xsl, ysl, dxsl, dysl):\n",
    "            ax.arrow(x, y, dx, dy, width=0.5, color=(1,0.5,0,0.3))\n",
    "        ax.imshow(image)\n",
    "        ax.grid(False)\n",
    "        fig.canvas.draw()\n",
    "        image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        # plt.close(fig)\n",
    "        if save:\n",
    "            plt.close(fig)\n",
    "            out_dir=os.path.join(save_folder,instance_token+'_'+sample_token)\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            im.imsave(os.path.join(out_dir,'bev'), image_from_plot)\n",
    "            # except:\n",
    "            #     continue\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "        key=instance_token+\"_\"+sample_token\n",
    "\n",
    "        missing_frames=token_dicts[key]\n",
    "        for idx,frame in enumerate(missing_frames):\n",
    "            yaw=pred[idx,-1].item()\n",
    "\n",
    "            global_coord=local_to_global(origin, tuple(pred[idx,:2].cpu().numpy()))\n",
    "            global_rotation=get_global_rotation(origin,yaw)\n",
    "\n",
    "            render_occ_anns(frame,helper.data,list(global_coord),global_rotation,save_folder,key,idx,save)\n",
    "        # break\n",
    "    return\n",
    "@torch.no_grad()\n",
    "def visualize_preparations(vis_dict, inputs: Dict,ground_truth: Dict,predictions: Dict,helper: PredictHelper_occ, selector,token_dicts,save_folder='./tmp', mode='refine',save=False):\n",
    "\n",
    "    batch_size=len(predictions['traj'])\n",
    "    for sample_id in range(batch_size):\n",
    "        if not selector[sample_id]:\n",
    "            continue\n",
    "        instance_token=inputs['instance_token'][sample_id]\n",
    "        sample_token=inputs['sample_token'][sample_id]\n",
    "        key=instance_token+\"_\"+sample_token\n",
    "        \n",
    "        future=inputs['target_agent_representation']['future']['traj'][sample_id]\n",
    "        mask_fut=inputs['target_agent_representation']['future']['mask'][sample_id]\n",
    "        hist=inputs['target_agent_representation']['history']['traj'][sample_id]\n",
    "        vis_dict[key]={\n",
    "            \"future\":future,\n",
    "            \"mask_fut\":mask_fut,\n",
    "            \"history\":hist,\n",
    "            \"origin\":inputs['origin'][sample_id],\n",
    "            \"traj\" :predictions['refined_traj'][sample_id],\n",
    "            \"yaw\" :predictions['refined_yaw'][sample_id],\n",
    "            \"pose_pred_mask\":predictions['mask'][sample_id],\n",
    "            \"gt\" : ground_truth['traj'][sample_id],\n",
    "            \"lanes\":inputs['map_representation']['lane_node_feats'][sample_id],\n",
    "            \"lanes_mask\":inputs['map_representation']['lane_node_feats'][sample_id],\n",
    "            \"pose_future_mask\":inputs['target_agent_representation']['future']['mask'],\n",
    "            \"pose_hist_mask\":inputs['target_agent_representation']['history']['mask'][sample_id]\n",
    "        }\n",
    "\n",
    "    return vis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list=trainer.dl.dataset.data_list\n",
    "token_dicts={}\n",
    "for token_dict in tokens_list:\n",
    "    key=token_dict['start']['ins_token']+\"_\"+token_dict['start']['sample_token']\n",
    "    token_dicts[key]=token_dict['missing_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 46\n",
      "1 46\n",
      "2 46\n",
      "3 46\n",
      "4 46\n",
      "5 46\n",
      "6 46\n",
      "7 46\n",
      "8 46\n",
      "9 46\n",
      "10 46\n",
      "11 46\n",
      "12 46\n",
      "13 46\n",
      "14 46\n",
      "15 46\n",
      "16 46\n",
      "17 46\n",
      "18 46\n",
      "19 46\n",
      "20 46\n",
      "21 46\n",
      "22 46\n",
      "23 46\n",
      "24 46\n",
      "25 46\n",
      "26 46\n",
      "27 46\n",
      "28 46\n",
      "29 46\n",
      "30 46\n",
      "31 46\n",
      "32 46\n",
      "33 46\n",
      "34 46\n",
      "35 46\n",
      "36 46\n",
      "37 46\n",
      "38 46\n",
      "39 46\n",
      "40 46\n",
      "41 46\n",
      "42 46\n",
      "43 46\n",
      "44 46\n",
      "45 46\n"
     ]
    }
   ],
   "source": [
    "import train_eval.utils as u\n",
    "helper= PredictHelper_occ(trainer.dl.dataset.helper.data)\n",
    "vis_dict={}\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(trainer.dl):\n",
    "        print(i,len(trainer.dl))\n",
    "        # torch.cuda.empty_cache()\n",
    "        # Load data\n",
    "        data = u.send_to_device(u.convert_double_to_float(data))\n",
    "        data_test=data['inputs']\n",
    "        gt_test=data['ground_truth']\n",
    "        \n",
    "        selectors=(torch.norm(gt_test['traj'][:,:,-1],1,dim=1).cpu()>1.5)*(torch.sum(torch.norm(gt_test['traj'][:,:,:2],2,dim=-1),dim=-1).cpu()>10.0)\n",
    "        predcitions=trainer.model(data_test)\n",
    "        # visualize(data_test,gt_test,predcitions,helper, selectors, token_dicts,save=True)\n",
    "        vis_dict=visualize_preparations(vis_dict, data_test, gt_test, predcitions, helper, selectors, token_dicts)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_sample_token=\"64daa0864dc04a1c94cd02328c449dec_d7400c505fbb47e29638c057780a696b\"\n",
    "vis_dict=vis_dict[vis_sample_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['drivable_area', 'ped_crossing']\n",
    "maps= load_all_maps(helper)\n",
    "colors = [(255, 255, 255), (119, 136, 153)]\n",
    "instance_token,sample_token=vis_sample_token.split[\"_\"]\n",
    "\n",
    "future=vis_dict['future']\n",
    "mask_fut=vis_dict['mask_fut']\n",
    "hist=vis_dict['history']\n",
    "nearest_idx=np.where(mask_fut[:, 0].cpu() == 0)[0][-1]\n",
    "prediction_horizon=future[nearest_idx,-1]\n",
    "sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "    \n",
    "map_name = helper.get_map_name_from_sample_token(sample_token)\n",
    "x, y = sample_annotation['translation'][:2]\n",
    "yaw = quaternion_yaw(Quaternion(sample_annotation['rotation']))\n",
    "yaw_corrected = correct_yaw(yaw)\n",
    "global_pose=(x,y,yaw_corrected)\n",
    "origin=tuple([vis_dict['origin'][0].item(),vis_dict['origin'][1].item(),vis_dict['origin'][2].item()])\n",
    "dist=LA.norm(future[0,:2].cpu(),ord=2)\n",
    "image_side_length = 2 * max(25,dist+10)\n",
    "image_side_length_pixels = 400\n",
    "resolution=image_side_length_pixels/image_side_length\n",
    "patchbox = get_patchbox(origin[0], origin[1], image_side_length)\n",
    "\n",
    "angle_in_degrees = angle_of_rotation(origin[2]) * 180 / np.pi\n",
    "\n",
    "canvas_size = (image_side_length_pixels, image_side_length_pixels)\n",
    "masks = maps[map_name].get_map_mask(patchbox, angle_in_degrees, layer_names, canvas_size=canvas_size)\n",
    "\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = os.path.join('vis_data','no_point'+'.json')\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data_list=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}\n",
    "for token_dict in data_list:\n",
    "    key=token_dict['start']['ins_token']+\"_\"+token_dict['start']['sample_token']\n",
    "    data_dict[key]=token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=data_test['target_agent_representation']['history']\n",
    "future=data_test['target_agent_representation']['future']\n",
    "concat_motion=data_test['target_agent_representation']['concat_motion']\n",
    "time_query=data_test['target_agent_representation']['time_query']\n",
    "refine_input=data_test['target_agent_representation']['refine_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('tmp'):\n",
    "    if filename.endswith('.png'):\n",
    "        key=filename[:-4]\n",
    "        token_dict=data_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predcitions=trainer.model(data_test)\n",
    "predcitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.2488,  6.2277, 26.1447,  3.5181,  6.0860, 11.3073,  5.1698,  4.6631,\n",
       "         9.5787,  2.2749,  8.1890,  2.4670,  3.8158, 12.2496,  4.5915],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.norm(gt_test['traj'][:,:,:2],2,dim=-1),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 3.915 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/home/stanliu/data/mnt/nuScenes/nuscenes', verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'e3d495d4ac534d54b321f50006683844',\n",
       " 'sample_token': 'ca9a282c9e77460f8360f564131a8af5',\n",
       " 'ego_pose_token': 'e3d495d4ac534d54b321f50006683844',\n",
       " 'calibrated_sensor_token': '1d31c729b073425e8e0202c5c6e66ee1',\n",
       " 'timestamp': 1532402927612460,\n",
       " 'fileformat': 'jpg',\n",
       " 'is_key_frame': True,\n",
       " 'height': 900,\n",
       " 'width': 1600,\n",
       " 'filename': 'samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402927612460.jpg',\n",
       " 'prev': '',\n",
       " 'next': '68e8e98cf7b0487baa139df808641db7',\n",
       " 'sensor_modality': 'camera',\n",
       " 'channel': 'CAM_FRONT'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_scene = nusc.scene[0]\n",
    "first_sample_token = my_scene['first_sample_token']\n",
    "my_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "\n",
    "sensor = 'CAM_FRONT'\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "cam_front_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_annotation_token = my_sample['anns'][18]\n",
    "my_annotation_metadata =  nusc.get('sample_annotation', my_annotation_token)\n",
    "\n",
    "\n",
    "nusc.render_annotation(my_annotation_token)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('offline_trk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2303c9ae493d0f9df0ec8e1fb2be14e51470c3887efa8666f9613d9dac54ced2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
