{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "from train_eval.trainer import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= torch.zeros(4,3,5,5)\n",
    "test[0,0,1,1]=1\n",
    "test.to_sparse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel_size=3\n",
    "channel=2\n",
    "unfold = nn.Unfold(kernel_size=(kernel_size, kernel_size), dilation=1, padding=kernel_size//2, stride=(1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=unfold(test).permute(0,2,1)\n",
    "b=a.view(a.shape[0],a.shape[1],channel,kernel_size**2)\n",
    "for i,j in enumerate(a):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unfolded_feature(feature,kernel,mask):\n",
    "    ## Input shape B,C,H,W\n",
    "    channel=feature.shape[1]\n",
    "    unfold = nn.Unfold(kernel_size=(kernel, kernel), dilation=1, padding=kernel//2, stride=(1, 1))\n",
    "    unfolded_feature=unfold(feature).permute(0,2,1) ## B,Number of slided window, channel*Number of elements in every window\n",
    "    # unfolded_feature=unfolded_feature.view(unfolded_feature.shape[0],unfolded_feature.shape[1],channel,kernel**2)\n",
    "    current_node_feat=feature.view(feature.shape[0],channel,-1).permute(0,2,1)## B,Number of slided window, channel\n",
    "    # unfolded_feature=(unfolded_feature*mask.unsqueeze(-1)).to_sparse()\n",
    "    # current_node_feat=(current_node_feat*mask.unsqueeze(-1)).to_sparse()\n",
    "    target_feat=[]\n",
    "    source_feat=[]\n",
    "    for idx,batch in enumerate(current_node_feat):\n",
    "        source_feat.append(batch[mask[idx]])\n",
    "        target_feat.append(unfolded_feature[idx][mask[idx]])\n",
    "    return source_feat,target_feat\n",
    "source_feat,target_feat=get_unfolded_feature(op,15,mask_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.blocks import LayerNorm\n",
    "deocde_block=nn.Sequential(\n",
    "                nn.Linear(2*48+2,48),\n",
    "                LayerNorm(48),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(48,48//2),\n",
    "                LayerNorm(48//2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(48//2,1)\n",
    "            ).to(op.device)\n",
    "softmax=nn.Softmax(dim=-1)\n",
    "def concat_feat(source_feat,target_feat,diff,channel):\n",
    "    diff=diff.unsqueeze(0).to(source_feat[0].device)\n",
    "    for idx,batch in enumerate(source_feat):\n",
    "        target_batch=target_feat[idx].view(target_feat[idx].shape[0],channel,-1).permute(0,2,1)\n",
    "        batch=batch.unsqueeze(-1).repeat(1,1,target_batch.shape[-2]).permute(0,2,1)\n",
    "        concat_feat=torch.cat((batch,target_batch,diff.repeat(batch.shape[0],1,1)),dim=-1)\n",
    "        connectivity=softmax(deocde_block(concat_feat).squeeze(-1))\n",
    "concat_feat(source_feat,target_feat,diff,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord,y_coord=torch.meshgrid(torch.arange(15//2,-((15//2)+1),-1),\n",
    "                                torch.arange(-(15//2),((15//2)+1),1))\n",
    "diff=torch.cat([x_coord.unsqueeze(0),y_coord.unsqueeze(0)],dim=0)\n",
    "diff=diff.view(2,-1).T\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/stanliu/code/pgp/PGP/configs/raster.yml\", 'r') as yaml_file:\n",
    "    cfg = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = cfg['dataset'] + '_' + cfg['agent_setting'] + '_' + cfg['input_representation']\n",
    "# cfg\n",
    "# cfg['encoder_args']\n",
    "# cfg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg, \"/home/stanliu/data/mnt/nuScenes/\", \"/home/stanliu/code/pgp/PGP/preprocess_raster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data in enumerate(trainer.tr_dl):\n",
    "    data_test=data['inputs']\n",
    "    gt_test=data['ground_truth']\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal=trainer.losses[0]\n",
    "# dri_loss=trainer.losses[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['ground_truth']['traj']\n",
    "map_representation = data_test['map_representation'][0]\n",
    "mask=data_test['map_representation'][1]\n",
    "\n",
    "\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "unfold = nn.Unfold(kernel_size=(7, 7), dilation=1, padding=7//2, stride=(1, 1))\n",
    "unfold(mask.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.RasterSampler import *\n",
    "sampler = Sampler(cfg['aggregator_args'],resolution=1.0,apply_mask=True)\n",
    "nodes_2D=sampler.sample_goals(mask)\n",
    "mask_under=sampler.sample_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "query_emb = nn.Linear(2, 32).to('cuda:0')\n",
    "for batch in nodes_2D:\n",
    "    node_emb=query_emb(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(255*test_htmap[0,:3].permute(1,2,0)*mask)\n",
    "plt.show()\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "plt.imshow(map_representation[0].permute(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(mask.squeeze(-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "window=create_window(7, 12)\n",
    "gaussian(7, 1)\n",
    "test_heatmap=F.conv2d(test_htmap, window, padding = 7//2, groups = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data_test['surrounding_agent_representation'][4,:3].permute(1,2,0))\n",
    "plt.show()\n",
    "# plt.imshow(test_heatmap[0,0])\n",
    "# plt.show()\n",
    "# data['ground_truth']['traj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings=trainer.model.encoder(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "target_agent_enc = encodings['target_agent_encoding']\n",
    "context_enc = encodings['context_encoding']\n",
    "if context_enc['combined'] is not None:\n",
    "    combined_enc, map_mask = context_enc['combined'], context_enc['map_masks'].bool()\n",
    "dim_reduction_block=nn.Sequential(\n",
    "            nn.Conv2d(544, 528, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(528),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(528, 512, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        ).to('cuda:0')\n",
    "query_emb = nn.Linear(2, 128).to('cuda:0')\n",
    "key_emb = nn.Linear(512, 128).to('cuda:0')\n",
    "val_emb = nn.Linear(512, 128).to('cuda:0')\n",
    "mha = nn.MultiheadAttention(128, 2).to('cuda:0')\n",
    "augmented_target_agent_enc = target_agent_enc.unsqueeze(2).unsqueeze(3).repeat(1,1,combined_enc.shape[-2],combined_enc.shape[-1])\n",
    "concatenated_encodings=torch.cat([combined_enc,augmented_target_agent_enc],dim=1)\n",
    "context_encoding = dim_reduction_block(concatenated_encodings)##Fuse agent feat with map feat by compressing the dimension (actually is linear layer)\n",
    "context_encoding = context_encoding.view(context_encoding.shape[0], context_encoding.shape[1], -1)## [Batch number, channel, H*W]\n",
    "context_encoding = context_encoding.permute(0, 2, 1)## [Batch number, H*W, channel]\n",
    "from models.library.RasterSampler import *\n",
    "t0=time.time()\n",
    "sampler = Sampler(cfg['aggregator_args'],resolution=1.0,apply_mask=True)\n",
    "nodes_2D=sampler.sample_goals(map_mask)\n",
    "mask_under=sampler.sample_mask(map_mask)\n",
    "\n",
    "nodes_2D=pad_tensor(nodes_2D)\n",
    "        \n",
    "query = query_emb(nodes_2D).permute(1,0,2)\n",
    "keys = key_emb(context_encoding).permute(1, 0, 2)\n",
    "vals = val_emb(context_encoding).permute(1, 0, 2)\n",
    "attn_output, _ = mha(query, keys, vals)\n",
    "time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def pad_tensor(node_list):\n",
    "    max_num = max([batch.size(0) for batch in node_list])\n",
    "    image_batch = []\n",
    "    for img in node_list:\n",
    "        image_batch.append(torch.cat((img, torch.zeros(max_num - img.size(0),img.size(1),device='cuda:0')), 0).unsqueeze(0))\n",
    "    return torch.cat(image_batch,dim=0).to('cuda:0')\n",
    "\n",
    "# pad_tensor(nodes_2D).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init(compensation,H,W):\n",
    "    compensation=compensation.long()\n",
    "    initial_pos=torch.zeros([H,W]).to(device)\n",
    "    initial_pos[compensation[0],compensation[1]]=1\n",
    "    return initial_pos.view(-1)\n",
    "compensate=torch.Tensor([100,61])\n",
    "init_pos=get_init(compensate,122,122)\n",
    "init_states=[]\n",
    "for batch in range(attn_output_weights.shape[0]):\n",
    "    init_states.append(init_pos[mask_under[batch]].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in init_states:\n",
    "    print(pad_tensor(batch).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent_enc = encodings['target_agent_encoding']\n",
    "context_enc = encodings['context_encoding']\n",
    "if context_enc['combined'] is not None:\n",
    "    combined_enc, map_mask = context_enc['combined'], context_enc['map_masks'].bool()\n",
    "dim_reduction_block=nn.Sequential(\n",
    "            nn.Conv2d(544, 528, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(528),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(528, 512, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        ).to('cuda:0')\n",
    "query_emb = nn.Linear(2, 128).to('cuda:0')\n",
    "key_emb = nn.Linear(512, 128).to('cuda:0')\n",
    "val_emb = nn.Linear(512, 128).to('cuda:0')\n",
    "mha = nn.MultiheadAttention(128, 2).to('cuda:0')\n",
    "augmented_target_agent_enc = target_agent_enc.unsqueeze(2).unsqueeze(3).repeat(1,1,combined_enc.shape[-2],combined_enc.shape[-1])\n",
    "concatenated_encodings=torch.cat([combined_enc,augmented_target_agent_enc],dim=1)\n",
    "context_encoding = dim_reduction_block(concatenated_encodings)##Fuse agent feat with map feat by compressing the dimension (actually is linear layer)\n",
    "context_encoding = context_encoding.view(context_encoding.shape[0], context_encoding.shape[1], -1)## [Batch number, channel, H*W]\n",
    "context_encoding = context_encoding.permute(0, 2, 1)## [Batch number, H*W, channel]\n",
    "from models.library.RasterSampler import *\n",
    "sampler = Sampler(cfg['aggregator_args'],resolution=1.0)\n",
    "nodes_2D=sampler.sample_goals().repeat(context_encoding.shape[0],1,1).type(torch.float32)\n",
    "t1=time.time()\n",
    "mask_under=sampler.sample_mask(map_mask)\n",
    "query = query_emb(nodes_2D).permute(1,0,2)\n",
    "keys = key_emb(context_encoding).permute(1, 0, 2)\n",
    "vals = val_emb(context_encoding).permute(1, 0, 2)\n",
    "attn_output, attn_output_weights = mha(query, keys, vals)\n",
    "time.time()-t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agg_feat=trainer.model.aggregator(encodings)\n",
    "op=agg_feat['agg_encoding']\n",
    "mask_map=agg_feat['under_sampled_mask']\n",
    "print(op.requires_grad)\n",
    "print(mask_map.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=trainer.model.decoder(agg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_weights = agg_feat['node_connectivity']\n",
    "init_states=agg_feat['initial_states']\n",
    "predictions=torch.empty([init_states.shape[0],0,init_states.shape[-1]],device=attn_output_weights.device)\n",
    "prev_states=init_states.unsqueeze(1)\n",
    "\n",
    "for step in range(12):\n",
    "    predictions=torch.cat((predictions,torch.bmm(prev_states,attn_output_weights)),dim=1)\n",
    "    prev_states=predictions[:,step].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predictions['pred']\n",
    "mask=predictions['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(pred,mask):\n",
    "    x_coord,y_coord=torch.meshgrid(torch.arange(0,122,1),\n",
    "                                torch.arange(0,122,1))\n",
    "    nodes_candidates=torch.cat((x_coord.unsqueeze(0),y_coord.unsqueeze(0)),dim=0).view(2,-1).T\n",
    "    nodes_2D=torch.zeros([mask.shape[0],pred.shape[-1],2])\n",
    "    for i in range(mask.shape[0]):\n",
    "        nodes_batch=nodes_candidates[mask[i]]\n",
    "        nodes_2D[i,:nodes_batch.shape[0]]=nodes_batch\n",
    "    return nodes_2D.int().permute(0,2,1).to(pred.device)\n",
    "nodes_2D=get_index(pred,mask)\n",
    "nodes_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(pred,nodes_2D,H,W):\n",
    "    dense_rep=torch.empty(0,pred.shape[1],H,W,device=pred.device)\n",
    "    for batch in range(pred.shape[0]):\n",
    "        batch_heatmap=torch.empty(0,H,W,device=pred.device)\n",
    "        for step in range(pred.shape[1]):\n",
    "            heatmap=torch.sparse_coo_tensor(nodes_2D[batch],pred[batch,step],(122,122))\n",
    "            batch_heatmap=torch.cat((batch_heatmap,heatmap.to_dense().unsqueeze(0)),dim=0)\n",
    "        dense_rep=torch.cat((dense_rep,batch_heatmap.unsqueeze(0)),dim=0)\n",
    "    return dense_rep\n",
    "dense_rep=get_dense(pred,nodes_2D,122,122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# loss=pred.sum()\n",
    "loss=focal.compute(predictions,gt_test)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions['pred']\n",
    "mask_da = predictions['mask'].view(-1,pred.shape[-2],pred.shape[-1]).unsqueeze(1)\n",
    "ground_truth = gt_test\n",
    "traj_gt = ground_truth['traj'] if type(ground_truth) == dict else ground_truth\n",
    "true_heatmap,gs_map = focal.generate_gtmap(traj_gt,pred.shape)\n",
    "gs_map=gs_map*mask_da\n",
    "mask = (true_heatmap == 1).float()\n",
    "pred_heatmap = torch.clamp(pred, min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.sum(\n",
    "                torch.pow(pred_heatmap - gs_map, 2) * (\n",
    "                mask * torch.log(pred_heatmap)\n",
    "                +\n",
    "                (1-mask) * (torch.pow(1 - gs_map, 4) * torch.log(1 - pred_heatmap))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dri_loss.compute(predictions,gt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions['pred']\n",
    "mask = predictions['mask'].view(-1,pred.shape[-2],pred.shape[-1]).unsqueeze(1)\n",
    "non_drivable_area_mask=~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_drivable_area_mask[0,0,0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(non_drivable_area_mask[0,0].cpu())\n",
    "plt.show()\n",
    "plt.imshow(torch.zeros_like(non_drivable_area_mask[0,0].cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "map_extent=[ -50, 50, -20, 80 ]\n",
    "img_size=[400,400]\n",
    "resolution = (map_extent[1] - map_extent[0]) /  img_size[1]\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=\"/home/stanliu/data/mnt/nuScenes/\", verbose=True)\n",
    "helper=PredictHelper(nusc)\n",
    "map_rasterizer = StaticLayerRasterizer(helper,\n",
    "                                        resolution=resolution,\n",
    "                                        meters_ahead=map_extent[3],\n",
    "                                        meters_behind=-map_extent[2],\n",
    "                                        meters_left=-map_extent[0],\n",
    "                                        meters_right=map_extent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "target_agent_representation = data_test['target_agent_representation']\n",
    "surrounding_agent_representation = data_test['surrounding_agent_representation']\n",
    "map_representation = data_test['map_representation'][0]\n",
    "mask= data_test['map_representation'][1].type(torch.bool)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.array(mask[idx]))\n",
    "plt.show()\n",
    "plt.imshow(np.array(map_representation[idx]).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models import resnet34\n",
    "# input = torch.cat((map_representation, surrounding_agent_representation), dim=1)\n",
    "# resnet_model = resnet34(pretrained=False)\n",
    "# conv1_new = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# modules = list(resnet_model.children())[:-2]\n",
    "\n",
    "# modules[0] = conv1_new\n",
    "# backbone = nn.Sequential(*modules)\n",
    "# data_test['target_agent_representation'].float().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoders.raster_encoder import *\n",
    "encoder=RasterEncoder(cfg['encoder_args'])\n",
    "# encodings=encoder.forward(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings['context_encoding'][\"combined\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.modules as nn\n",
    "fake_map_encodings=torch.randn(32, 512, 16,16)\n",
    "fake_agent_input=torch.randn(32, 32).unsqueeze(2).unsqueeze(3).repeat(1,1,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_encodings=torch.cat([fake_map_encodings,fake_agent_input],dim=1)\n",
    "conv1d1=nn.Conv2d(544, 528, kernel_size=1, stride=1, bias=False)\n",
    "conv1d2=nn.Conv2d(528, 512, kernel_size=1, stride=1, bias=False)\n",
    "test_dim_reduction=nn.Sequential(conv1d1,nn.BatchNorm2d(528),nn.ReLU(),conv1d2,nn.BatchNorm2d(512),nn.ReLU())\n",
    "fake_feature=test_dim_reduction(concatenated_encodings)\n",
    "fake_feature=fake_feature.view(fake_feature.shape[0], fake_feature.shape[1], -1).permute(0, 2, 1)\n",
    "\n",
    "fake_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_test=final_convs(transpose_convs(fake_feature))\n",
    "# augmented_mask=mask.unsqueeze(-1)\n",
    "print(upsampled_test[:,:,::2,::2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from model.decode import generic_decode\n",
    "\n",
    "# from utils.image import gaussian_radius, draw_umich_gaussian\n",
    "\n",
    "# from model.ConvGRU import ConvGRU\n",
    "\n",
    "from einops import rearrange as rearr, repeat\n",
    "\n",
    "from spatial_correlation_sampler import spatial_correlation_sample\n",
    "\n",
    "\n",
    "class LocalWalk(nn.Module):\n",
    "    def __init__(self, topk=0, radius=0.05, temp=0.05, pad_value=0,\n",
    "            broadcast_val=False, corr_module=True):\n",
    "        super(LocalWalk, self).__init__()\n",
    "\n",
    "        self.topk = topk\n",
    "        self.radius = radius\n",
    "        self.vals = {}\n",
    "        self.idxmaps = {}\n",
    "        self.temp = temp\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.broadcast_val = broadcast_val\n",
    "\n",
    "        self.corr_module = corr_module\n",
    "\n",
    "    def get_identity_label(self, keys):\n",
    "        '''\n",
    "        returns 1 x H*W x H x W as reshaped H*W x H*W identity matrix\n",
    "        '''\n",
    "        B, C, H, W = keys.shape\n",
    "        name = f\"{H}_{W}\"\n",
    "        if name not in self.vals:\n",
    "            vals = self.distance_field(H, W).flatten(0, 1)\n",
    "            vals = (vals == 0).float() ##Returns an identity matrix, which is composed of multiple matrices.\n",
    "            # The i th matrix has an element 1 at the i ith position, the rest places are all zeros.\n",
    "            vals = repeat(vals, 'n h w -> b n h w', b=B if not self.broadcast_val else 1)## Repeat for batch number times\n",
    "            self.vals[name] = vals.to(keys.device)\n",
    "            print('created vals')\n",
    "\n",
    "        return self.vals[name]\n",
    "\n",
    "    def knn(self, A):\n",
    "        if self.pad_value == 0 or self.topk > 0:\n",
    "            with torch.no_grad():\n",
    "                mask = (A == self.pad_value)\n",
    "                if self.topk > 0:\n",
    "                  mask |= (A < A.topk(k=self.topk, dim=-1)[0].min(-1, keepdim=True)[0])\n",
    "            A[mask] = -10\n",
    "\n",
    "        return A\n",
    "\n",
    "    def distance_field(self, H, W, p=2):\n",
    "        gx, gy = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))\n",
    "        D = ( (gx[None, None, :, :] - gx[:, :, None, None]).abs()**p + (gy[None, None, :, :] - gy[:, :, None, None]).abs()**p ).float() #** (1/p)\n",
    "        return D\n",
    "\n",
    "    def make_scatter_map(self, keys, kH, kW):\n",
    "        B, C, H, W = keys.shape\n",
    "        name = f\"{H}_{W}_{kH}_{kW}\"\n",
    "        if name not in self.idxmaps:\n",
    "            idx_map = torch.arange(H*W).view(H, W)[None, None].float()\n",
    "            idx_map = torch.nn.functional.unfold(idx_map, kernel_size=(kH, kW), stride=1, padding=(kH//2, kW//2))\n",
    "            idx_map = rearr(idx_map, 'b n hw -> b hw n')\n",
    "            idx_map = idx_map.clamp(min=0).long()\n",
    "            self.idxmaps[name] = idx_map.to(keys.device)\n",
    "            print('created idx map')\n",
    "\n",
    "        return self.idxmaps[name]\n",
    "\n",
    "    def forward(self, query, keys, val=None):\n",
    "        '''\n",
    "        assumes q, k, v: B D N\n",
    "        '''\n",
    "\n",
    "        B, C, H, W = keys.shape\n",
    "        kW = kH = int(H * self.radius) * 2 + 1\n",
    "\n",
    "        val = self.make_scatter_map(keys, kH, kW)## Returns the indices of elements inside the sliding windows of all steps\n",
    "        ## The sliding window has size (kH,kW), the input is keys.\n",
    "        # out = self.get_identity_label(keys) * 0##Why all zeros?\n",
    "        out = torch.zeros([B,H*W,H,W]).float()\n",
    "        out = repeat(out, '1 n h w -> b (h w) n', b=B) if out.shape[0] == 1 else \\\n",
    "              rearr(out, 'b n h w -> b (h w) n') ##Repeat for batch number\n",
    "\n",
    "        if self.corr_module:\n",
    "            att = spatial_correlation_sample(query,\n",
    "                               keys,\n",
    "                               kernel_size=1,\n",
    "                               patch_size=kH,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               dilation=1,\n",
    "                               dilation_patch=1) / self.temp\n",
    "            att = rearr(att, 'b p1 p2 h w -> b h w (p1 p2)')##Local connectivity, for each node (pixel)\n",
    "            ## calculate its node similarity with nearby nodes\n",
    "\n",
    "        A = self.knn(att)\n",
    "\n",
    "        A = torch.exp(rearr(A, 'b h w n -> b (h w) n'))\n",
    "        out.scatter_add_(2, val.to(A.device).expand_as(A), A)\n",
    "        val = rearr(out, 'b (h w) n -> b n h w', h=H)\n",
    "\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor=torch.randn([32,20,100,100])\n",
    "test_walker=LocalWalk(radius=0.2)\n",
    "test_walker.forward(test_tensor,test_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.RasterSampler import *\n",
    "sampler=Sampler(cfg['train_set_args'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mask=torch.randn([32,488,488]).ge(0)\n",
    "nodes_2D=sampler.sample_goals().repeat(32,1,1).type(torch.float32)\n",
    "mask_under=(sampler.sample_mask(mask))\n",
    "attn_mask=~mask_under.unsqueeze(-1).repeat(2,1,256)\n",
    "print(attn_mask.shape)\n",
    "print(mask.shape)\n",
    "print(nodes_2D.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.array(mask[0]))\n",
    "plt.show()\n",
    "plt.imshow(np.array(~mask_under[0].view(122,122)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_feature.shape\n",
    "lin1=nn.Linear(2, 128)\n",
    "lin2=nn.Linear(512, 128)\n",
    "print(lin1(nodes_2D.type(torch.float32)).shape)\n",
    "print(lin2(fake_feature.type(torch.float32)).shape)\n",
    "print(attn_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention():\n",
    "    \"\"\"\n",
    "    Test fusing 2D goal encodings and map encodings with attention mechanism. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        args to include\n",
    "\n",
    "        enc_size: int Dimension of encodings generated by encoder\n",
    "        emb_size: int Size of embeddings used for queries, keys and values\n",
    "        num_heads: int Number of attention heads\n",
    "\n",
    "        \"\"\"\n",
    "        self.query_emb = nn.Linear(2, 128)\n",
    "        self.key_emb = nn.Linear(512, 128)\n",
    "        self.val_emb = nn.Linear(512, 128)\n",
    "        self.mha = nn.MultiheadAttention(128, 2)\n",
    "\n",
    "    def forward(self, query_goals, map_features, attn_mask) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for attention aggregator\n",
    "        \"\"\"\n",
    "\n",
    "        query = self.query_emb(query_goals).permute(1,0,2)\n",
    "        keys = self.key_emb(map_features).permute(1,0,2)\n",
    "        vals = self.val_emb(map_features).permute(1,0,2)\n",
    "        op, _ = self.mha(query, keys, vals, attn_mask=attn_mask)\n",
    "        # op = op.squeeze(0)\n",
    "        # op = torch.cat((target_agent_enc, op), dim=-1)\n",
    "\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.library.blocks import TransposeCNNBlock\n",
    "trans_conv=nn.Sequential([\n",
    "            TransposeCNNBlock(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            TransposeCNNBlock(in_channels=256, out_channels=128, kernel_size=3, stride=2, output_padding=0),\n",
    "            TransposeCNNBlock(in_channels=128, out_channels=64, kernel_size=3, stride=2, output_padding=0),\n",
    "            TransposeCNNBlock(in_channels=64, out_channels=32, kernel_size=3, stride=2, output_padding=1)\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pgp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "593542b65644041049b318720dfeebe70f9074a000f082b969de259755a1a643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
